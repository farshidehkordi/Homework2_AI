{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOue/AC3v5Yj6QPg5Ah1ZQZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farshidehkordi/Homework2_AI/blob/main/TP_2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w_-clTlXHs-",
        "outputId": "889225a7-b501-48a6-ca38-e9a22cd3bfd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# List files in the current directory\n",
        "print(os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djuzPz1AXKEF",
        "outputId": "e6b7128a-bed2-450e-f027-29d225760c15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'codeTP2.zip', 'drive', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ./codeTP2.zip\n",
        "# extract files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ni9yeOHXUPk",
        "outputId": "a270cc99-98a7-4d95-f521-72ecba0d2e3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./codeTP2.zip\n",
            "   creating: code/data-q2/\n",
            "  inflating: code/data-q2/test.txt   \n",
            "  inflating: code/data-q2/train.txt  \n",
            "  inflating: code/q2-RNN.py          \n",
            "  inflating: code/question1.py       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install poutyne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0pSnbdcZztO",
        "outputId": "04b785fe-8859-41f1-90d7-0e08a3b3bbb4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting poutyne\n",
            "  Downloading Poutyne-1.17.1-py3-none-any.whl (213 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/213.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m204.8/213.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.5/213.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from poutyne) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from poutyne) (2.2.1+cu121)\n",
            "Collecting torchmetrics (from poutyne)\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->poutyne)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->poutyne)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->poutyne)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->poutyne)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->poutyne)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->poutyne)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->poutyne)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->poutyne)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->poutyne)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->poutyne)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->poutyne)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->poutyne)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics->poutyne) (24.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->poutyne)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics->poutyne) (67.7.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->poutyne) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->poutyne) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, poutyne\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 poutyne-1.17.1 torchmetrics-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial architecture with the WordClassifier class"
      ],
      "metadata": {
        "id": "3dp4oOG0ltR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "\n",
        "\n",
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "\n",
        "# Define embedding_size and hidden_size\n",
        "embedding_size = 10\n",
        "hidden_size = 10\n",
        "\n",
        "\n",
        "# bidirectional\n",
        "class WordClassifier(nn.Module):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_size, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=True)  # Bidirectional RNN\n",
        "        self.mapping_layer = nn.Linear(hidden_size * 2, num_classes)  # Multiply hidden size by 2 for bidirectional RNN\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Inputs: (batch_size, seq_len)\n",
        "        embedded = self.embedding(inputs)  # Shape: (batch_size, seq_len, embedding_size)\n",
        "\n",
        "        output, _ = self.rnn(embedded)  # Shape: (batch_size, seq_len, hidden_size * 2) because it's bidirectional\n",
        "\n",
        "        # Take the last output of the sequence\n",
        "        last_output = output[:, -1, :]\n",
        "\n",
        "        logits = self.mapping_layer(last_output)  # Shape: (batch_size, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class WordClassifierHandlingPadding(WordClassifier):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__(vocab, embedding_size, hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Inputs: (batch_size, seq_len)\n",
        "        embedded = self.embedding(inputs)  # Shape: (batch_size, seq_len, embedding_size)\n",
        "        output, _ = self.rnn(embedded)     # Shape: (batch_size, seq_len, hidden_size)\n",
        "        last_output = output[:, -1, :]     # Take the last output of the sequence\n",
        "        logits = self.fc(last_output)      # Shape: (batch_size, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "def vectorize_dataset(dataset, char_to_idx, class_to_idx):\n",
        "    vectorized_dataset = list()\n",
        "    for word, lang in dataset:\n",
        "        label = class_to_idx[lang]\n",
        "        vectorized_word = list()\n",
        "        for char in word:\n",
        "            vectorized_word.append(char_to_idx.get(char, 1))  # Get the char index otherwise set to unknown char\n",
        "        vectorized_dataset.append((vectorized_word, label))\n",
        "    return vectorized_dataset\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    examples = list()\n",
        "    with open(filename) as fhandle:\n",
        "        for line in fhandle:\n",
        "            examples.append(line[:-1].split())\n",
        "    return examples\n",
        "\n",
        "\n",
        "def create_indexes(examples):\n",
        "    char_to_idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    class_to_idx = {}\n",
        "\n",
        "    for word, lang in examples:\n",
        "        if lang not in class_to_idx:\n",
        "            class_to_idx[lang] = len(class_to_idx)\n",
        "        for char in word:\n",
        "            if char not in char_to_idx:\n",
        "                char_to_idx[char] = len(char_to_idx)\n",
        "    return char_to_idx, class_to_idx\n",
        "\n",
        "\n",
        "def make_max_padded_dataset(dataset):\n",
        "    max_length = max([len(w) for w, l in dataset])\n",
        "    tensor_dataset = torch.zeros((len(dataset), max_length), dtype=torch.long)\n",
        "    labels = list()\n",
        "    for i, (word, label) in enumerate(dataset):\n",
        "        tensor_dataset[i, :len(word)] = torch.LongTensor(word)\n",
        "        labels.append(label)\n",
        "    return tensor_dataset, torch.LongTensor(labels)\n",
        "\n",
        "def collate_examples(samples):\n",
        "    # Get the length of the longest sequence in the batch\n",
        "    max_len = max(len(word) for word, _ in samples)\n",
        "\n",
        "    # Initialize lists to store padded sequences and labels\n",
        "    padded_sequences = []\n",
        "    labels = []\n",
        "\n",
        "    # Pad each sequence to have the same length as the longest sequence in the batch\n",
        "    for word, label in samples:\n",
        "        # Convert the tensor to a Python list\n",
        "        word_list = word.tolist()\n",
        "\n",
        "        # Pad the sequence with the padding token (\"<pad>\") to match the maximum length\n",
        "        padded_sequence = torch.tensor(word_list + [0] * (max_len - len(word_list)), dtype=torch.long)\n",
        "        padded_sequences.append(padded_sequence)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert the lists to PyTorch tensors\n",
        "    padded_sequences_tensor = torch.stack(padded_sequences)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return padded_sequences_tensor, labels_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    batch_size = 128\n",
        "    training_set = load_data(\"./code/data-q2/train.txt\")\n",
        "    test_set = load_data(\"./code/data-q2/test.txt\")\n",
        "\n",
        "\n",
        "    char_to_idx, class_to_idx = create_indexes(training_set)\n",
        "\n",
        "    vectorized_train = vectorize_dataset(training_set, char_to_idx, class_to_idx)\n",
        "    vectorized_test = vectorize_dataset(test_set, char_to_idx, class_to_idx)\n",
        "\n",
        "    X_train, y_train = make_max_padded_dataset(vectorized_train)\n",
        "    X_test, y_test = make_max_padded_dataset(vectorized_test)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # 1: Create a simple network that takes inputs of fixed length (max length)\n",
        "    network = WordClassifier(char_to_idx, 10, 10, len(class_to_idx))\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"1 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "    # 2: Faites en sorte que le padding soit fait \"on batch\"\n",
        "    # Le tout devrait se passer dans la fonction collate_examples\n",
        "    train_loader = DataLoader(vectorized_train, batch_size=128, shuffle=True, collate_fn=collate_examples)\n",
        "    test_loader = DataLoader(vectorized_test, batch_size=128, collate_fn=collate_examples)\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"2 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzBQjOrgmh6k",
        "outputId": "aa7b7b33-6003-459e-f17d-4edcf10318a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 Train steps: 1696 14.34s loss: 2.294531 acc: 12.554144                               \n",
            "Epoch: 2/5 Train steps: 1696 15.03s loss: 2.290366 acc: 13.035685                                \n",
            "Epoch: 3/5 Train steps: 1696 14.15s loss: 2.290358 acc: 13.035685                                \n",
            "Epoch: 4/5 Train steps: 1696 13.93s loss: 2.290369 acc: 13.035685                               \n",
            "Epoch: 5/5 Train steps: 1696 13.97s loss: 2.290363 acc: 13.035685                                \n",
            "Test steps: 424 1.15s test_loss: 2.291355 test_acc: 13.044200                                \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:1 - Loss: 2.2913547344853225\tAcc:13.044199506378764\n"
          ]
        }
      ]
    }
  ]
}