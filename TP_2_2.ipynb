{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoVwBz2eZtoAr7SpbX55KE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farshidehkordi/Homework2_AI/blob/main/TP_2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "\n",
        "\n",
        "class WordClassifier(nn.Module):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_size, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True)\n",
        "        self.mapping_layer = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embedding(inputs)  # Shape: (batch_size, seq_len, embedding_size)\n",
        "        output, _ = self.rnn(embedded)     # Shape: (batch_size, seq_len, hidden_size)\n",
        "        last_output = output[:, -1, :]     # Take the last output of the sequence\n",
        "        logits = self.mapping_layer(last_output)  # Shape: (batch_size, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class WordClassifierHandlingPadding(WordClassifier):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__(vocab, embedding_size, hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        pass\n",
        "\n",
        "\n",
        "def vectorize_dataset(dataset, char_to_idx, class_to_idx):\n",
        "    vectorized_dataset = list()\n",
        "    for word, lang in dataset:\n",
        "        label = class_to_idx[lang]\n",
        "        vectorized_word = list()\n",
        "        for char in word:\n",
        "            vectorized_word.append(char_to_idx.get(char, 1))  # Get the char index otherwise set to unknown char\n",
        "        vectorized_dataset.append((vectorized_word, label))\n",
        "    return vectorized_dataset\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    examples = list()\n",
        "    with open(filename) as fhandle:\n",
        "        for line in fhandle:\n",
        "            examples.append(line[:-1].split())\n",
        "    return examples\n",
        "\n",
        "\n",
        "def create_indexes(examples):\n",
        "    char_to_idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    class_to_idx = {}\n",
        "\n",
        "    for word, lang in examples:\n",
        "        if lang not in class_to_idx:\n",
        "            class_to_idx[lang] = len(class_to_idx)\n",
        "        for char in word:\n",
        "            if char not in char_to_idx:\n",
        "                char_to_idx[char] = len(char_to_idx)\n",
        "    return char_to_idx, class_to_idx\n",
        "\n",
        "\n",
        "def make_max_padded_dataset(dataset):\n",
        "    max_length = max([len(w) for w, l in dataset])\n",
        "    tensor_dataset = torch.zeros((len(dataset), max_length), dtype=torch.long)\n",
        "    labels = list()\n",
        "    for i, (word, label) in enumerate(dataset):\n",
        "        tensor_dataset[i, :len(word)] = torch.LongTensor(word)\n",
        "        labels.append(label)\n",
        "    return tensor_dataset, torch.LongTensor(labels)\n",
        "\n",
        "\n",
        "def collate_examples(samples):\n",
        "    pass\n",
        "\n",
        "\n",
        "def main():\n",
        "    batch_size = 128\n",
        "    training_set = load_data(\"./data/train.txt\")\n",
        "    test_set = load_data(\"./data/test.txt\")\n",
        "\n",
        "    char_to_idx, class_to_idx = create_indexes(training_set)\n",
        "\n",
        "    vectorized_train = vectorize_dataset(training_set, char_to_idx, class_to_idx)\n",
        "    vectorized_test = vectorize_dataset(test_set, char_to_idx, class_to_idx)\n",
        "\n",
        "    X_train, y_train = make_max_padded_dataset(vectorized_train)\n",
        "    X_test, y_test = make_max_padded_dataset(vectorized_test)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    network = WordClassifier(char_to_idx, 10, 10, len(class_to_idx))\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"1 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "JzBQjOrgmh6k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}