{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farshidehkordi/Homework2_AI/blob/main/TP2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djuzPz1AXKEF",
        "outputId": "bc64a6d8-3043-410c-d126-c79e2050f71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# List files in the current directory\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ni9yeOHXUPk",
        "outputId": "46ac29f5-da0b-4365-b487-d6d43f544cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./codeTP2.zip\n",
            "   creating: code/data-q2/\n",
            "  inflating: code/data-q2/test.txt   \n",
            "  inflating: code/data-q2/train.txt  \n",
            "  inflating: code/q2-RNN.py          \n",
            "  inflating: code/question1.py       \n"
          ]
        }
      ],
      "source": [
        "!unzip ./codeTP2.zip\n",
        "# extract files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0pSnbdcZztO",
        "outputId": "4dfa6674-e1f7-4190-ca7f-ad168c63384d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting poutyne\n",
            "  Downloading Poutyne-1.17.1-py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.5/213.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from poutyne) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from poutyne) (2.2.1+cu121)\n",
            "Collecting torchmetrics (from poutyne)\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->poutyne)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->poutyne)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->poutyne)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->poutyne)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->poutyne)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->poutyne)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->poutyne)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->poutyne)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->poutyne)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->poutyne)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->poutyne)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->poutyne)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics->poutyne) (24.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->poutyne)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics->poutyne) (67.7.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->poutyne) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->poutyne) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, poutyne\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 poutyne-1.17.1 torchmetrics-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install poutyne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dp4oOG0ltR5"
      },
      "source": [
        "Initial architecture with the bidirectional WordClassifier class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzBQjOrgmh6k",
        "outputId": "f3196af0-a257-4a47-c226-e9734eb70c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 Train steps: 1696 20.32s loss: 2.294531 acc: 12.554144                                \n",
            "Epoch: 2/5 Train steps: 1696 20.40s loss: 2.290366 acc: 13.035685                                \n",
            "Epoch: 3/5 Train steps: 1696 20.02s loss: 2.290358 acc: 13.035685                                \n",
            "Epoch: 4/5 Train steps: 1696 24.37s loss: 2.290369 acc: 13.035685                                \n",
            "Epoch: 5/5 Train steps: 1696 35.12s loss: 2.290363 acc: 13.035685                                \n",
            "Test steps: 424 1.60s test_loss: 2.291355 test_acc: 13.044200                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:1 - Loss: 2.2913547406727623\tAcc:13.044199506378764\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "\n",
        "# Define embedding_size and hidden_size\n",
        "embedding_size = 10\n",
        "hidden_size = 10\n",
        "\n",
        "\n",
        "# bidirectional\n",
        "class WordClassifier(nn.Module):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_size, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=True)  # Bidirectional RNN\n",
        "        self.mapping_layer = nn.Linear(hidden_size * 2, num_classes)  # Multiply hidden size by 2 for bidirectional RNN\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Inputs: (batch_size, seq_len)\n",
        "        embedded = self.embedding(inputs)  # Shape: (batch_size, seq_len, embedding_size)\n",
        "\n",
        "        output, _ = self.rnn(embedded)  # Shape: (batch_size, seq_len, hidden_size * 2) because it's bidirectional\n",
        "\n",
        "        # Take the last output of the sequence\n",
        "        last_output = output[:, -1, :]\n",
        "\n",
        "        logits = self.mapping_layer(last_output)  # Shape: (batch_size, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "class WordClassifierHandlingPadding(WordClassifier):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__(vocab, embedding_size, hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Inputs: (batch_size, seq_len)\n",
        "        embedded = self.embedding(inputs)  # Shape: (batch_size, seq_len, embedding_size)\n",
        "        output, _ = self.rnn(embedded)     # Shape: (batch_size, seq_len, hidden_size)\n",
        "        last_output = output[:, -1, :]     # Take the last output of the sequence\n",
        "        logits = self.fc(last_output)      # Shape: (batch_size, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "def vectorize_dataset(dataset, char_to_idx, class_to_idx):\n",
        "    vectorized_dataset = list()\n",
        "    for word, lang in dataset:\n",
        "        label = class_to_idx[lang]\n",
        "        vectorized_word = list()\n",
        "        for char in word:\n",
        "            vectorized_word.append(char_to_idx.get(char, 1))  # Get the char index otherwise set to unknown char\n",
        "        vectorized_dataset.append((vectorized_word, label))\n",
        "    return vectorized_dataset\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    examples = list()\n",
        "    with open(filename) as fhandle:\n",
        "        for line in fhandle:\n",
        "            examples.append(line[:-1].split())\n",
        "    return examples\n",
        "\n",
        "\n",
        "def create_indexes(examples):\n",
        "    char_to_idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    class_to_idx = {}\n",
        "\n",
        "    for word, lang in examples:\n",
        "        if lang not in class_to_idx:\n",
        "            class_to_idx[lang] = len(class_to_idx)\n",
        "        for char in word:\n",
        "            if char not in char_to_idx:\n",
        "                char_to_idx[char] = len(char_to_idx)\n",
        "    return char_to_idx, class_to_idx\n",
        "\n",
        "\n",
        "def make_max_padded_dataset(dataset):\n",
        "    max_length = max([len(w) for w, l in dataset])\n",
        "    tensor_dataset = torch.zeros((len(dataset), max_length), dtype=torch.long)\n",
        "    labels = list()\n",
        "    for i, (word, label) in enumerate(dataset):\n",
        "        tensor_dataset[i, :len(word)] = torch.LongTensor(word)\n",
        "        labels.append(label)\n",
        "    return tensor_dataset, torch.LongTensor(labels)\n",
        "\n",
        "def collate_examples(samples):\n",
        "    # Get the length of the longest sequence in the batch\n",
        "    max_len = max(len(word) for word, _ in samples)\n",
        "\n",
        "    # Initialize lists to store padded sequences and labels\n",
        "    padded_sequences = []\n",
        "    labels = []\n",
        "\n",
        "    # Pad each sequence to have the same length as the longest sequence in the batch\n",
        "    for word, label in samples:\n",
        "        # Convert the tensor to a Python list\n",
        "        word_list = word.tolist()\n",
        "\n",
        "        # Pad the sequence with the padding token (\"<pad>\") to match the maximum length\n",
        "        padded_sequence = torch.tensor(word_list + [0] * (max_len - len(word_list)), dtype=torch.long)\n",
        "        padded_sequences.append(padded_sequence)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert the lists to PyTorch tensors\n",
        "    padded_sequences_tensor = torch.stack(padded_sequences)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return padded_sequences_tensor, labels_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    batch_size = 128\n",
        "    training_set = load_data(\"./code/data-q2/train.txt\")\n",
        "    test_set = load_data(\"./code/data-q2/test.txt\")\n",
        "\n",
        "\n",
        "    char_to_idx, class_to_idx = create_indexes(training_set)\n",
        "\n",
        "    vectorized_train = vectorize_dataset(training_set, char_to_idx, class_to_idx)\n",
        "    vectorized_test = vectorize_dataset(test_set, char_to_idx, class_to_idx)\n",
        "\n",
        "    X_train, y_train = make_max_padded_dataset(vectorized_train)\n",
        "    X_test, y_test = make_max_padded_dataset(vectorized_test)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # 1: Create a simple network that takes inputs of fixed length (max length)\n",
        "    network = WordClassifier(char_to_idx, 10, 10, len(class_to_idx))\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"1 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "\n",
        "\n",
        "class WordClassifier(nn.Module):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_size, padding_idx=0)\n",
        "        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=5, batch_first=True)\n",
        "        self.mapping_layer = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embedding(inputs)  # Shape: (batch_size, seq_len, embedding_size)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        last_output = output[:, -1, :]  # Take the last output of each sequence\n",
        "        logits = self.mapping_layer(last_output)  # Shape: (batch_size, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "class WordClassifierHandlingPadding(WordClassifier):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__(vocab, embedding_size, hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # TODO: Gestion des exemples a longueur variable\n",
        "        # Servez-vous de la fonction pack_padded_sequence\n",
        "        pass\n",
        "\n",
        "\n",
        "def vectorize_dataset(dataset, char_to_idx, class_to_idx):\n",
        "    vectorized_dataset = list()\n",
        "    for word, lang in dataset:\n",
        "        label = class_to_idx[lang]\n",
        "        vectorized_word = list()\n",
        "        for char in word:\n",
        "            vectorized_word.append(char_to_idx.get(char, 1))  # Get the char index otherwise set to unknown char\n",
        "        vectorized_dataset.append((vectorized_word, label))\n",
        "    return vectorized_dataset\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    examples = list()\n",
        "    with open(filename) as fhandle:\n",
        "        for line in fhandle:\n",
        "            examples.append(line[:-1].split())\n",
        "    return examples\n",
        "\n",
        "\n",
        "def create_indexes(examples):\n",
        "    char_to_idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    class_to_idx = {}\n",
        "\n",
        "    for word, lang in examples:\n",
        "        if lang not in class_to_idx:\n",
        "            class_to_idx[lang] = len(class_to_idx)\n",
        "        for char in word:\n",
        "            if char not in char_to_idx:\n",
        "                char_to_idx[char] = len(char_to_idx)\n",
        "    return char_to_idx, class_to_idx\n",
        "\n",
        "\n",
        "def make_max_padded_dataset(dataset):\n",
        "    max_length = max([len(w) for w, l in dataset])\n",
        "    tensor_dataset = torch.zeros((len(dataset), max_length), dtype=torch.long)\n",
        "    labels = list()\n",
        "    for i, (word, label) in enumerate(dataset):\n",
        "        tensor_dataset[i, :len(word)] = torch.LongTensor(word)\n",
        "        labels.append(label)\n",
        "    return tensor_dataset, torch.LongTensor(labels)\n",
        "\n",
        "\n",
        "def collate_examples(samples):\n",
        "    # TODO: Cette fonction devrait faire du \"padding on batch\"\n",
        "    # i.e. utiliser la longueur de la séquence la plus longue\n",
        "    # de la batch et non pas du jeu de données complet.\n",
        "    pass\n",
        "\n",
        "\n",
        "def main():\n",
        "    batch_size = 128\n",
        "    training_set = load_data(\"./code/data-q2/train.txt\")\n",
        "    test_set = load_data(\"./code/data-q2/test.txt\")\n",
        "\n",
        "    char_to_idx, class_to_idx = create_indexes(training_set)\n",
        "\n",
        "    vectorized_train = vectorize_dataset(training_set, char_to_idx, class_to_idx)\n",
        "    vectorized_test = vectorize_dataset(test_set, char_to_idx, class_to_idx)\n",
        "\n",
        "    X_train, y_train = make_max_padded_dataset(vectorized_train)\n",
        "    X_test, y_test = make_max_padded_dataset(vectorized_test)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # 1: Créez un réseau simple qui prend en entré des exemples de longueur fixes (max length)\n",
        "    network = WordClassifier(char_to_idx, 10, 10, len(class_to_idx))\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"1 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "    # 2: Faites en sorte que le padding soit fait \"on batch\"\n",
        "    # Le tout devrait se passer dans la fonction collate_examples\n",
        "    train_loader = DataLoader(vectorized_train, batch_size=128, shuffle=True, collate_fn=collate_examples)\n",
        "    test_loader = DataLoader(vectorized_test, batch_size=128, collate_fn=collate_examples)\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"2 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "    # 3: Créez une architecture qui gère convenablement des séquences de longueur différentes\n",
        "    network = WordClassifierHandlingPadding(char_to_idx, 10, 10, len(class_to_idx))\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"3 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "s8OOxjvQHDiU",
        "outputId": "6847e5cc-8907-4857-a274-b654fc68c60d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 Train steps: 1696 43.79s loss: 2.293860 acc: 12.796067                                \n",
            "Epoch: 2/5 Train steps: 1696 41.17s loss: 2.290389 acc: 13.035224                                \n",
            "Epoch: 3/5 Train steps: 1696 48.36s loss: 2.290400 acc: 13.035224                                \n",
            "Epoch: 4/5 Train steps: 1696 40.54s loss: 2.290409 acc: 13.035224                                \n",
            "Epoch: 5/5 Train steps: 1696 43.97s loss: 2.290370 acc: 13.035224                                \n",
            "Test steps: 424 2.58s test_loss: 2.291327 test_acc: 13.042356                                \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:1 - Loss: 2.291327490590195\tAcc:13.042356324309239\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable NoneType object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d2e35beab47c>\u001b[0m in \u001b[0;36m<cell line: 131>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-d2e35beab47c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cross_entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2 - Loss: {}\\tAcc:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/poutyne/framework/model.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, train_generator, valid_generator, epochs, steps_per_epoch, validation_steps, batches_per_step, initial_epoch, verbose, progress_options, callbacks)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_generator_n_batches_per_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_per_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_generator_one_batch_per_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/poutyne/framework/model.py\u001b[0m in \u001b[0;36m_fit_generator_one_batch_per_step\u001b[0;34m(self, epoch_iterator, callback_list)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_step_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_step_iterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_training_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_step_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmKRW7gSZIOF"
      },
      "source": [
        "Adding the collate_examples function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "# Define embedding_size and hidden_size\n",
        "embedding_size = 10\n",
        "hidden_size = 10\n",
        "\n",
        "\n",
        "# Bidirectional RNN\n",
        "class WordClassifier(nn.Module):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes, dropout_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_size, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout_prob)  # Dropout layer\n",
        "        self.mapping_layer = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embedding(inputs)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)  # Apply dropout\n",
        "        logits = self.mapping_layer(last_output)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def vectorize_dataset(dataset, char_to_idx, class_to_idx):\n",
        "    vectorized_dataset = list()\n",
        "    for word, lang in dataset:\n",
        "        label = class_to_idx[lang]\n",
        "        vectorized_word = [char_to_idx.get(char, char_to_idx[\"<unk>\"]) for char in word]\n",
        "        vectorized_dataset.append((vectorized_word, label))\n",
        "    return vectorized_dataset\n",
        "\n",
        "\n",
        "def collate_examples(samples, padding_value=0):\n",
        "    max_len = max(len(word) for word, _ in samples)\n",
        "    padded_sequences = []\n",
        "    labels = []\n",
        "    for word, label in samples:\n",
        "        padded_sequence = torch.tensor(word + [padding_value] * (max_len - len(word)), dtype=torch.long)\n",
        "        padded_sequences.append(padded_sequence)\n",
        "        labels.append(label)\n",
        "    padded_sequences_tensor = torch.stack(padded_sequences)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "    return padded_sequences_tensor, labels_tensor\n",
        "\n",
        "\n",
        "class LearningRateSchedulerCallback:\n",
        "    def __init__(self, scheduler):\n",
        "        self.scheduler = scheduler\n",
        "        self.model = None  # Initialize model attribute\n",
        "\n",
        "    def set_model(self, model):  # Add set_model method\n",
        "        self.model = model\n",
        "\n",
        "    def on_epoch_begin(self, epoch_number: int, logs: Dict):\n",
        "        pass\n",
        "\n",
        "    def on_epoch_end(self, epoch_number: int, logs: Dict):\n",
        "        self.scheduler.step()\n",
        "\n",
        "\n",
        "def main():\n",
        "    batch_size = 128\n",
        "    training_set = load_data(\"./code/data-q2/train.txt\")\n",
        "    test_set = load_data(\"./code/data-q2/test.txt\")\n",
        "\n",
        "    char_to_idx, class_to_idx = create_indexes(training_set)\n",
        "\n",
        "    vectorized_train = vectorize_dataset(training_set, char_to_idx, class_to_idx)\n",
        "    vectorized_test = vectorize_dataset(test_set, char_to_idx, class_to_idx)\n",
        "\n",
        "    X_train, y_train = make_max_padded_dataset(vectorized_train)\n",
        "    X_test, y_test = make_max_padded_dataset(vectorized_test)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Define the network\n",
        "    network = WordClassifier(char_to_idx, embedding_size, hidden_size, len(class_to_idx))\n",
        "\n",
        "    # Define optimizer with weight decay\n",
        "    optimizer = SGD(network.parameters(), lr=0.1, weight_decay=1e-5)  # Adding weight decay\n",
        "\n",
        "    # Define learning rate scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=0.1)  # Optionally adjust learning rate\n",
        "\n",
        "    # 1: Create a simple network that takes inputs of fixed length (max length)\n",
        "    model = Model(network, optimizer, 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5, callbacks=[LearningRateSchedulerCallback(scheduler)])\n",
        "\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"1 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "    # 2: Ensure padding is done \"on batch\"\n",
        "    train_loader = DataLoader(vectorized_train, batch_size=128, shuffle=True, collate_fn=collate_examples)\n",
        "    test_loader = DataLoader(vectorized_test, batch_size=128, collate_fn=collate_examples)\n",
        "    model = Model(network, optimizer, 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5, callbacks=[LearningRateSchedulerCallback(scheduler)])\n",
        "\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"2 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "la4Mmkk4iO_U",
        "outputId": "8790ceaf-515a-4f06-f261-a31bcbe2ee9c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-543a4a182f1d>\u001b[0m in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-543a4a182f1d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mtraining_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./code/data-q2/train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./code/data-q2/test.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWoQqnq-gTM9"
      },
      "source": [
        "Added WordClassifierHandlingPadding function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIklQzmsiVaZ",
        "outputId": "4f6548a2-20b2-4acd-fa6d-188c3f4d650b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 Train steps: 1696 20.10s loss: 2.303213 acc: 11.609496                                \n",
            "Epoch: 2/5 Train steps: 1696 19.42s loss: 2.292226 acc: 13.107109                                \n",
            "Epoch: 3/5 Train steps: 1696 19.95s loss: 2.098547 acc: 19.032588                                \n",
            "Epoch: 4/5 Train steps: 1696 19.06s loss: 2.180794 acc: 16.213389                                \n",
            "Epoch: 5/5 Train steps: 1696 21.92s loss: 1.986190 acc: 21.363796                                \n",
            "Test steps: 424 1.74s test_loss: 1.786515 test_acc: 25.050688                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:2 - Loss: 1.7865145067761359\tAcc:25.05068751077908\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "\n",
        "# Define embedding_size and hidden_size\n",
        "embedding_size = 10\n",
        "hidden_size = 10\n",
        "\n",
        "\n",
        "# bidirectional\n",
        "class WordClassifier(nn.Module):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes, dropout_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_size, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout_prob)  # Dropout layer\n",
        "        self.mapping_layer = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embedding(inputs)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)  # Apply dropout\n",
        "        logits = self.mapping_layer(last_output)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "class WordClassifierHandlingPadding(WordClassifier):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__(vocab, embedding_size, hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # If inputs is a tuple, assume it contains sequences and lengths\n",
        "        if isinstance(inputs, tuple):\n",
        "            sequences, lengths = inputs\n",
        "        # If inputs is not a tuple, assume it contains only sequences and compute lengths\n",
        "        else:\n",
        "            sequences = inputs\n",
        "            lengths = torch.count_nonzero(sequences, dim=1).cpu().tolist()\n",
        "\n",
        "        # Embed the sequences\n",
        "        embedded = self.embedding(sequences)  # Shape: (batch_size, seq_len, embedding_size)\n",
        "\n",
        "        # Pack the embedded sequences to handle padding\n",
        "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass the packed sequences through the RNN\n",
        "        packed_output, _ = self.rnn(packed_embedded)\n",
        "\n",
        "        # Unpack the packed output\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        # Take the last output of each sequence\n",
        "        last_output = output[:, -1, :]\n",
        "\n",
        "        # Pass the last output through the mapping layer\n",
        "        logits = self.mapping_layer(last_output)  # Shape: (batch_size, num_classes)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def vectorize_dataset(dataset, char_to_idx, class_to_idx):\n",
        "    vectorized_dataset = list()\n",
        "    for word, lang in dataset:\n",
        "        label = class_to_idx[lang]\n",
        "        vectorized_word = list()\n",
        "        for char in word:\n",
        "            # Check if the character is in the vocabulary\n",
        "            if char in char_to_idx:\n",
        "                vectorized_word.append(char_to_idx[char])\n",
        "            else:\n",
        "                # If the character is not in the vocabulary, use the unknown token index\n",
        "                vectorized_word.append(char_to_idx[\"<unk>\"])\n",
        "        vectorized_dataset.append((vectorized_word, label))\n",
        "    return vectorized_dataset\n",
        "\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    examples = list()\n",
        "    with open(filename) as fhandle:\n",
        "        for line in fhandle:\n",
        "            examples.append(line[:-1].split())\n",
        "    return examples\n",
        "\n",
        "\n",
        "def create_indexes(examples):\n",
        "    char_to_idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    class_to_idx = {}\n",
        "\n",
        "    for word, lang in examples:\n",
        "        if lang not in class_to_idx:\n",
        "            class_to_idx[lang] = len(class_to_idx)\n",
        "        for char in word:\n",
        "            if char not in char_to_idx:\n",
        "                char_to_idx[char] = len(char_to_idx)\n",
        "    return char_to_idx, class_to_idx\n",
        "\n",
        "\n",
        "def make_max_padded_dataset(dataset):\n",
        "    max_length = max([len(w) for w, l in dataset])\n",
        "    tensor_dataset = torch.zeros((len(dataset), max_length), dtype=torch.long)\n",
        "    labels = list()\n",
        "    for i, (word, label) in enumerate(dataset):\n",
        "        tensor_dataset[i, :len(word)] = torch.LongTensor(word)\n",
        "        labels.append(label)\n",
        "    return tensor_dataset, torch.LongTensor(labels)\n",
        "\n",
        "\n",
        "\n",
        "def collate_examples(samples, padding_value=0):\n",
        "    # Find the maximum sequence length within the batch\n",
        "    max_len = max(len(word) for word, _ in samples)\n",
        "\n",
        "    padded_sequences = []\n",
        "    labels = []\n",
        "\n",
        "    # Pad each sequence to have the same length as the longest sequence in the batch\n",
        "    for word, label in samples:\n",
        "        # Pad the sequence to match the maximum length\n",
        "        padded_sequence = torch.tensor(word + [padding_value] * (max_len - len(word)), dtype=torch.long)\n",
        "        padded_sequences.append(padded_sequence)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert the lists to PyTorch tensors\n",
        "    padded_sequences_tensor = torch.stack(padded_sequences)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return padded_sequences_tensor, labels_tensor\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    batch_size = 128\n",
        "    training_set = load_data(\"./code/data-q2/train.txt\")\n",
        "    test_set = load_data(\"./code/data-q2/test.txt\")\n",
        "\n",
        "    char_to_idx, class_to_idx = create_indexes(training_set)\n",
        "\n",
        "    vectorized_train = vectorize_dataset(training_set, char_to_idx, class_to_idx)\n",
        "    vectorized_test = vectorize_dataset(test_set, char_to_idx, class_to_idx)\n",
        "\n",
        "    X_train, y_train = make_max_padded_dataset(vectorized_train)\n",
        "    X_test, y_test = make_max_padded_dataset(vectorized_test)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Define the network\n",
        "    # Define the network\n",
        "    network = WordClassifier(char_to_idx, embedding_size, hidden_size, len(class_to_idx), dropout_prob=0.7)\n",
        "\n",
        "    # Define optimizer with weight decay\n",
        "    optimizer = SGD(network.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "\n",
        "    # 2: Ensure padding is done \"on batch\"\n",
        "    train_loader = DataLoader(vectorized_train, batch_size=128, shuffle=True, collate_fn=collate_examples)\n",
        "    test_loader = DataLoader(vectorized_test, batch_size=128, collate_fn=collate_examples)\n",
        "    model = Model(network, optimizer, 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"2 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "from torch.optim import SGD\n",
        "from typing import Dict\n",
        "\n",
        "# Define embedding_size and hidden_size\n",
        "embedding_size = 10\n",
        "hidden_size = 10\n",
        "\n",
        "\n",
        "# Bidirectional RNN\n",
        "class WordClassifier(nn.Module):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes, dropout_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_size, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout_prob)  # Dropout layer\n",
        "        self.mapping_layer = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embedding(inputs)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)  # Apply dropout\n",
        "        logits = self.mapping_layer(last_output)\n",
        "        return logits\n",
        "\n",
        "def vectorize_dataset(dataset, char_to_idx, class_to_idx):\n",
        "    vectorized_dataset = list()\n",
        "    for word, lang in dataset:\n",
        "        label = class_to_idx[lang]\n",
        "        vectorized_word = [char_to_idx.get(char, char_to_idx[\"<unk>\"]) for char in word]\n",
        "        vectorized_dataset.append((vectorized_word, label))\n",
        "    return vectorized_dataset\n",
        "\n",
        "def create_indexes(examples):\n",
        "    char_to_idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    class_to_idx = {}\n",
        "\n",
        "    for word, lang in examples:\n",
        "        if lang not in class_to_idx:\n",
        "            class_to_idx[lang] = len(class_to_idx)\n",
        "        for char in word:\n",
        "            if char not in char_to_idx:\n",
        "                char_to_idx[char] = len(char_to_idx)\n",
        "    return char_to_idx, class_to_idx\n",
        "\n",
        "\n",
        "def make_max_padded_dataset(dataset):\n",
        "    max_length = max([len(w) for w, l in dataset])\n",
        "    tensor_dataset = torch.zeros((len(dataset), max_length), dtype=torch.long)\n",
        "    labels = list()\n",
        "    for i, (word, label) in enumerate(dataset):\n",
        "        tensor_dataset[i, :len(word)] = torch.LongTensor(word)\n",
        "        labels.append(label)\n",
        "    return tensor_dataset, torch.LongTensor(labels)\n",
        "\n",
        "def collate_examples(samples):\n",
        "    # Get the length of the longest sequence in the batch\n",
        "    max_len = max(len(word) for word, _ in samples)\n",
        "\n",
        "    # Initialize lists to store padded sequences and labels\n",
        "    padded_sequences = []\n",
        "    labels = []\n",
        "\n",
        "    # Pad each sequence to have the same length as the longest sequence in the batch\n",
        "    for word, label in samples:\n",
        "        # Pad the sequence with the padding token (\"\") to match the maximum length\n",
        "        padded_sequence = torch.tensor(word + [0] * (max_len - len(word)), dtype=torch.long)\n",
        "        padded_sequences.append(padded_sequence)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert the lists to PyTorch tensors\n",
        "    padded_sequences_tensor = torch.stack(padded_sequences)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return padded_sequences_tensor, labels_tensor\n",
        "\n",
        "def load_data(filename):\n",
        "    examples = list()\n",
        "    with open(filename) as fhandle:\n",
        "        for line in fhandle:\n",
        "            examples.append(line[:-1].split())\n",
        "    return examples\n",
        "\n",
        "def main():\n",
        "    batch_size = 128\n",
        "    training_set = load_data(\"./code/data-q2/train.txt\")\n",
        "    test_set = load_data(\"./code/data-q2/test.txt\")\n",
        "\n",
        "    char_to_idx, class_to_idx = create_indexes(training_set)\n",
        "\n",
        "    vectorized_train = vectorize_dataset(training_set, char_to_idx, class_to_idx)\n",
        "    vectorized_test = vectorize_dataset(test_set, char_to_idx, class_to_idx)\n",
        "\n",
        "    X_train, y_train = make_max_padded_dataset(vectorized_train)\n",
        "    X_test, y_test = make_max_padded_dataset(vectorized_test)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "    # Define the network\n",
        "    network = WordClassifier(char_to_idx, embedding_size, hidden_size, len(class_to_idx), dropout_prob=0.5)\n",
        "\n",
        "    # Define optimizer with fixed learning rate\n",
        "    optimizer = SGD(network.parameters(), lr=0.01, weight_decay=1e-5)  # Adding weight decay\n",
        " # 2: Faites en sorte que le padding soit fait \"on batch\"\n",
        "    # Le tout devrait se passer dans la fonction collate_examples\n",
        "    train_loader = DataLoader(vectorized_train, batch_size=128, shuffle=True, collate_fn=collate_examples)\n",
        "    test_loader = DataLoader(vectorized_test, batch_size=128, collate_fn=collate_examples)\n",
        "    model = Model(network, optimizer , 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"2 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt7BB2M7kspT",
        "outputId": "45803e18-c2d6-4eb8-a980-6a6bcb564582"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/4 Train steps: 1696 18.05s loss: 2.300271 acc: 11.928833                                \n",
            "Epoch: 2/4 Train steps: 1696 19.12s loss: 2.291906 acc: 13.011723                                \n",
            "Epoch: 3/4 Train steps: 1696 17.75s loss: 2.091115 acc: 19.043647                                \n",
            "Epoch: 4/4 Train steps: 1696 20.02s loss: 1.916296 acc: 21.884043                               \n",
            "Test steps: 424 1.70s test_loss: 2.615184 test_acc: 11.075681                                \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:2 - Loss: 2.6151835787119198\tAcc:11.075681056829367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "\n",
        "# Define embedding_size and hidden_size\n",
        "embedding_size = 10\n",
        "hidden_size = 10\n",
        "\n",
        "\n",
        "# bidirectional\n",
        "class WordClassifier(nn.Module):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_size, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=True)  # Bidirectional RNN\n",
        "        self.mapping_layer = nn.Linear(hidden_size * 2, num_classes)  # Multiply hidden size by 2 for bidirectional RNN\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Inputs: (batch_size, seq_len)\n",
        "        embedded = self.embedding(inputs)  # Shape: (batch_size, seq_len, embedding_size)\n",
        "\n",
        "        output, _ = self.rnn(embedded)  # Shape: (batch_size, seq_len, hidden_size * 2) because it's bidirectional\n",
        "\n",
        "        # Take the last output of the sequence\n",
        "        last_output = output[:, -1, :]\n",
        "\n",
        "        logits = self.mapping_layer(last_output)  # Shape: (batch_size, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class WordClassifierHandlingPadding(WordClassifier):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__(vocab, embedding_size, hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # If inputs is a tuple, assume it contains sequences and lengths\n",
        "        if isinstance(inputs, tuple):\n",
        "            sequences, lengths = inputs\n",
        "        # If inputs is not a tuple, assume it contains only sequences and compute lengths\n",
        "        else:\n",
        "            sequences = inputs\n",
        "            lengths = torch.count_nonzero(sequences, dim=1).cpu().tolist()\n",
        "\n",
        "        # Embed the sequences\n",
        "        embedded = self.embedding(sequences)  # Shape: (batch_size, seq_len, embedding_size)\n",
        "\n",
        "        # Pack the embedded sequences to handle padding\n",
        "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass the packed sequences through the RNN\n",
        "        packed_output, _ = self.rnn(packed_embedded)\n",
        "\n",
        "        # Unpack the packed output\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        # Take the last output of each sequence\n",
        "        last_output = output[:, -1, :]\n",
        "\n",
        "        # Pass the last output through the mapping layer\n",
        "        logits = self.mapping_layer(last_output)  # Shape: (batch_size, num_classes)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def vectorize_dataset(dataset, char_to_idx, class_to_idx):\n",
        "    vectorized_dataset = list()\n",
        "    for word, lang in dataset:\n",
        "        label = class_to_idx[lang]\n",
        "        vectorized_word = list()\n",
        "        for char in word:\n",
        "            vectorized_word.append(char_to_idx.get(char, 1))  # Get the char index otherwise set to unknown char\n",
        "        vectorized_dataset.append((vectorized_word, label))\n",
        "    return vectorized_dataset\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    examples = list()\n",
        "    with open(filename) as fhandle:\n",
        "        for line in fhandle:\n",
        "            examples.append(line[:-1].split())\n",
        "    return examples\n",
        "\n",
        "\n",
        "def create_indexes(examples):\n",
        "    char_to_idx = {\"\": 0, \"\": 1}\n",
        "    class_to_idx = {}\n",
        "\n",
        "    for word, lang in examples:\n",
        "        if lang not in class_to_idx:\n",
        "            class_to_idx[lang] = len(class_to_idx)\n",
        "        for char in word:\n",
        "            if char not in char_to_idx:\n",
        "                char_to_idx[char] = len(char_to_idx)\n",
        "    return char_to_idx, class_to_idx\n",
        "\n",
        "\n",
        "def make_max_padded_dataset(dataset):\n",
        "    max_length = max([len(w) for w, l in dataset])\n",
        "    tensor_dataset = torch.zeros((len(dataset), max_length), dtype=torch.long)\n",
        "    labels = list()\n",
        "    for i, (word, label) in enumerate(dataset):\n",
        "        tensor_dataset[i, :len(word)] = torch.LongTensor(word)\n",
        "        labels.append(label)\n",
        "    return tensor_dataset, torch.LongTensor(labels)\n",
        "\n",
        "import torch\n",
        "\n",
        "def collate_examples(samples):\n",
        "    # Get the length of the longest sequence in the batch\n",
        "    max_len = max(len(word) for word, _ in samples)\n",
        "\n",
        "    # Initialize lists to store padded sequences and labels\n",
        "    padded_sequences = []\n",
        "    labels = []\n",
        "\n",
        "    # Pad each sequence to have the same length as the longest sequence in the batch\n",
        "    for word, label in samples:\n",
        "        # Pad the sequence with the padding token (\"\") to match the maximum length\n",
        "        padded_sequence = torch.tensor(word + [0] * (max_len - len(word)), dtype=torch.long)\n",
        "        padded_sequences.append(padded_sequence)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert the lists to PyTorch tensors\n",
        "    padded_sequences_tensor = torch.stack(padded_sequences)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return padded_sequences_tensor, labels_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    batch_size = 128\n",
        "    training_set = load_data(\"./code/data-q2/train.txt\")\n",
        "    test_set = load_data(\"./code/data-q2/test.txt\")\n",
        "\n",
        "\n",
        "    char_to_idx, class_to_idx = create_indexes(training_set)\n",
        "\n",
        "    vectorized_train = vectorize_dataset(training_set, char_to_idx, class_to_idx)\n",
        "    vectorized_test = vectorize_dataset(test_set, char_to_idx, class_to_idx)\n",
        "\n",
        "    X_train, y_train = make_max_padded_dataset(vectorized_train)\n",
        "    X_test, y_test = make_max_padded_dataset(vectorized_test)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # 1: Create a simple network that takes inputs of fixed length (max length)\n",
        "    network = WordClassifier(char_to_idx, 10, 10, len(class_to_idx))\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"1 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "    # 2: Faites en sorte que le padding soit fait \"on batch\"\n",
        "    # Le tout devrait se passer dans la fonction collate_examples\n",
        "    train_loader = DataLoader(vectorized_train, batch_size=128, shuffle=True, collate_fn=collate_examples)\n",
        "    test_loader = DataLoader(vectorized_test, batch_size=128, collate_fn=collate_examples)\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"2 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "\n",
        "   # 3: Créez une architecture qui gère convenablement des séquences de longueur différentes\n",
        "    network = WordClassifierHandlingPadding(char_to_idx, 10, 10, len(class_to_idx))\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"3 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88w0kgf870X7",
        "outputId": "209a2960-06b8-45d4-bb76-081d15c16894"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 Train steps: 1696 20.88s loss: 2.292462 acc: 12.977163                                \n",
            "Epoch: 2/5 Train steps: 1696 21.00s loss: 2.290368 acc: 13.035224                                \n",
            "Epoch: 3/5 Train steps: 1696 20.56s loss: 2.290344 acc: 13.035224                                \n",
            "Epoch: 4/5 Train steps: 1696 19.87s loss: 2.290351 acc: 13.035685                               \n",
            "Epoch: 5/5 Train steps: 1696 21.16s loss: 2.290365 acc: 13.035224                                \n",
            "Test steps: 424 1.55s test_loss: 2.291345 test_acc: 13.042356                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:1 - Loss: 2.29134544172416\tAcc:13.042356324309239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 Train steps: 1696 15.15s loss: 2.289814 acc: 13.139826                               \n",
            "Epoch: 2/5 Train steps: 1696 16.29s loss: 2.098270 acc: 18.663945                                \n",
            "Epoch: 3/5 Train steps: 1696 16.44s loss: 1.803160 acc: 23.791772                                \n",
            "Epoch: 4/5 Train steps: 1696 15.95s loss: 2.244017 acc: 15.482093                                \n",
            "Epoch: 5/5 Train steps: 1696 15.46s loss: 2.290105 acc: 13.133375                                \n",
            "Test steps: 424 1.60s test_loss: 2.290914 test_acc: 13.169536                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:2 - Loss: 2.2909140156193155\tAcc:13.169535885348658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 Train steps: 1696 22.01s loss: 2.294385 acc: 12.392402                               \n",
            "Epoch: 2/5 Train steps: 1696 21.68s loss: 2.289981 acc: 13.100197                               \n",
            "Epoch: 3/5 Train steps: 1696 21.60s loss: 2.289349 acc: 13.195123                                \n",
            "Epoch: 4/5 Train steps: 1696 21.06s loss: 2.289090 acc: 13.256871                               \n",
            "Epoch: 5/5 Train steps: 1696 22.14s loss: 2.288703 acc: 13.308941                                \n",
            "Test steps: 424 1.72s test_loss: 2.289563 test_acc: 13.353854                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:3 - Loss: 2.2895626768579125\tAcc:13.35385409230114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "z-y3CrAwA2O0",
        "outputId": "1d8c2257-54ca-45f0-c864-70d568c55a50"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Sequence Length: 2.0\n",
            "Median Sequence Length: 2.0\n",
            "Minimum Sequence Length: 2\n",
            "Maximum Sequence Length: 2\n",
            "Standard Deviation of Sequence Lengths: 0.0\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "dataset = load_data(\"./code/data-q2/train.txt\")\n",
        "# Assuming dataset is a list of sequences, where each sequence is a list of tokens\n",
        "# Calculate sequence lengths\n",
        "sequence_lengths = [len(sequence) for sequence in dataset]\n",
        "\n",
        "# Visualize the distribution using a histogram\n",
        "plt.hist(sequence_lengths, bins=20)\n",
        "plt.title('Distribution of Sequence Lengths')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Compute descriptive statistics\n",
        "mean_length = np.mean(sequence_lengths)\n",
        "median_length = np.median(sequence_lengths)\n",
        "min_length = np.min(sequence_lengths)\n",
        "max_length = np.max(sequence_lengths)\n",
        "std_length = np.std(sequence_lengths)\n",
        "\n",
        "print(f\"Mean Sequence Length: {mean_length}\")\n",
        "print(f\"Median Sequence Length: {median_length}\")\n",
        "print(f\"Minimum Sequence Length: {min_length}\")\n",
        "print(f\"Maximum Sequence Length: {max_length}\")\n",
        "print(f\"Standard Deviation of Sequence Lengths: {std_length}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "5oEgWgvECpkW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "d884bae6-3816-4b3f-9dba-c01c198ac126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 Train steps: 1696 19.91s loss: 2.294531 acc: 12.554144                                \n",
            "Epoch: 2/5 Train steps: 1696 19.92s loss: 2.290366 acc: 13.035685                                \n",
            "Epoch: 3/5 Train steps: 1696 19.81s loss: 2.290358 acc: 13.035685                                \n",
            "Epoch: 4/5 Train steps: 1696 19.21s loss: 2.290369 acc: 13.035685                                \n",
            "Epoch: 5/5 Train steps: 1696 20.50s loss: 2.290363 acc: 13.035685                                \n",
            "Test steps: 424 1.73s test_loss: 2.291355 test_acc: 13.044200                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:1 - Loss: 2.2913547406727623\tAcc:13.044199506378764\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'char_to_idx' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-294886b10491>\u001b[0m in \u001b[0;36m<cell line: 197>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-294886b10491>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cross_entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2 - Loss: {}\\tAcc:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/poutyne/framework/model.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, train_generator, valid_generator, epochs, steps_per_epoch, validation_steps, batches_per_step, initial_epoch, verbose, progress_options, callbacks)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_generator_n_batches_per_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_per_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_generator_one_batch_per_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/poutyne/framework/model.py\u001b[0m in \u001b[0;36m_fit_generator_one_batch_per_step\u001b[0;34m(self, epoch_iterator, callback_list)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_step_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_step_iterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_training_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_step_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/poutyne/framework/iterators.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mtime_since_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_get_step_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/poutyne/framework/iterators.py\u001b[0m in \u001b[0;36mcycle\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Equivalent to itertools cycle, without any extra memory requirement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-75-294886b10491>\u001b[0m in \u001b[0;36mcollate_examples\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;31m# 1: Créez un réseau simple qui prend en entré des exemples de longueur fixes (max length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cross_entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'char_to_idx' is not defined"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define embedding_size and hidden_size\n",
        "embedding_size = 10\n",
        "hidden_size = 10\n",
        "\n",
        "\n",
        "# bidirectional\n",
        "class WordClassifier(nn.Module):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_size, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=True)  # Bidirectional RNN\n",
        "        self.mapping_layer = nn.Linear(hidden_size * 2, num_classes)  # Multiply hidden size by 2 for bidirectional RNN\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Inputs: (batch_size, seq_len)\n",
        "        embedded = self.embedding(inputs)  # Shape: (batch_size, seq_len, embedding_size)\n",
        "\n",
        "        output, _ = self.rnn(embedded)  # Shape: (batch_size, seq_len, hidden_size * 2) because it's bidirectional\n",
        "\n",
        "        # Take the last output of the sequence\n",
        "        last_output = output[:, -1, :]\n",
        "\n",
        "        logits = self.mapping_layer(last_output)  # Shape: (batch_size, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class WordClassifierHandlingPadding(WordClassifier):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__(vocab, embedding_size, hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # If inputs is a tuple, assume it contains sequences and lengths\n",
        "        if isinstance(inputs, tuple):\n",
        "            sequences, lengths = inputs\n",
        "        # If inputs is not a tuple, assume it contains only sequences and compute lengths\n",
        "        else:\n",
        "            sequences = inputs\n",
        "            lengths = torch.count_nonzero(sequences, dim=1).cpu().tolist()\n",
        "\n",
        "        # Embed the sequences\n",
        "        embedded = self.embedding(sequences)  # Shape: (batch_size, seq_len, embedding_size)\n",
        "\n",
        "        # Pack the embedded sequences to handle padding\n",
        "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass the packed sequences through the RNN\n",
        "        packed_output, _ = self.rnn(packed_embedded)\n",
        "\n",
        "        # Unpack the packed output\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        # Take the last output of each sequence\n",
        "        last_output = output[:, -1, :]\n",
        "\n",
        "        # Pass the last output through the mapping layer\n",
        "        logits = self.mapping_layer(last_output)  # Shape: (batch_size, num_classes)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "def vectorize_dataset(dataset, char_to_idx, class_to_idx):\n",
        "    vectorized_dataset = list()\n",
        "    for word, lang in dataset:\n",
        "        label = class_to_idx[lang]\n",
        "        vectorized_word = list()\n",
        "        for char in word:\n",
        "            vectorized_word.append(char_to_idx.get(char, 1))  # Get the char index otherwise set to unknown char\n",
        "        vectorized_dataset.append((vectorized_word, label))\n",
        "    return vectorized_dataset\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    examples = list()\n",
        "    with open(filename) as fhandle:\n",
        "        for line in fhandle:\n",
        "            examples.append(line[:-1].split())\n",
        "    return examples\n",
        "\n",
        "\n",
        "def create_indexes(examples):\n",
        "    char_to_idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    class_to_idx = {}\n",
        "\n",
        "    for word, lang in examples:\n",
        "        if lang not in class_to_idx:\n",
        "            class_to_idx[lang] = len(class_to_idx)\n",
        "        for char in word:\n",
        "            if char not in char_to_idx:\n",
        "                char_to_idx[char] = len(char_to_idx)\n",
        "    return char_to_idx, class_to_idx\n",
        "\n",
        "\n",
        "def make_max_padded_dataset(dataset):\n",
        "    max_length = max([len(w) for w, l in dataset])\n",
        "    tensor_dataset = torch.zeros((len(dataset), max_length), dtype=torch.long)\n",
        "    labels = list()\n",
        "    for i, (word, label) in enumerate(dataset):\n",
        "        tensor_dataset[i, :len(word)] = torch.LongTensor(word)\n",
        "        labels.append(label)\n",
        "    return tensor_dataset, torch.LongTensor(labels)\n",
        "\n",
        "import torch\n",
        "\n",
        "def collate_examples(samples):\n",
        "    # Get the length of the longest sequence in the batch\n",
        "    max_len = max(len(word) for word, _ in samples)\n",
        "\n",
        "    # Initialize lists to store padded sequences and labels\n",
        "    padded_sequences = []\n",
        "    labels = []\n",
        "    # 1: Créez un réseau simple qui prend en entré des exemples de longueur fixes (max length)\n",
        "    network = WordClassifier(char_to_idx, 10, 10, len(class_to_idx))\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"1 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "    # 2: Faites en sorte que le padding soit fait \"on batch\"\n",
        "    # Le tout devrait se passer dans la fonction collate_examples\n",
        "    train_loader = DataLoader(vectorized_train, batch_size=128, shuffle=True, collate_fn=collate_examples)\n",
        "    test_loader = DataLoader(vectorized_test, batch_size=128, collate_fn=collate_examples)\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"2 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "    # Pad each sequence to have the same length as the longest sequence in the batch\n",
        "    for word, label in samples:\n",
        "        # Pad the sequence with the padding token (\"<pad>\") to match the maximum length\n",
        "        padded_sequence = torch.tensor(word + [0] * (max_len - len(word)), dtype=torch.long)\n",
        "        padded_sequences.append(padded_sequence)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert the lists to PyTorch tensors\n",
        "    padded_sequences_tensor = torch.stack(padded_sequences)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return padded_sequences_tensor, labels_tensor\n",
        "\n",
        "\n",
        "def main():\n",
        "    batch_size = 128\n",
        "    training_set = load_data(\"./code/data-q2/train.txt\")\n",
        "    test_set = load_data(\"./code/data-q2/test.txt\")\n",
        "\n",
        "    char_to_idx, class_to_idx = create_indexes(training_set)\n",
        "\n",
        "    vectorized_train = vectorize_dataset(training_set, char_to_idx, class_to_idx)\n",
        "    vectorized_test = vectorize_dataset(test_set, char_to_idx, class_to_idx)\n",
        "\n",
        "    X_train, y_train = make_max_padded_dataset(vectorized_train)\n",
        "    X_test, y_test = make_max_padded_dataset(vectorized_test)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # 1: Créez un réseau simple qui prend en entré des exemples de longueur fixes (max length)\n",
        "    network = WordClassifier(char_to_idx, 10, 10, len(class_to_idx))\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"1 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "    # 2: Faites en sorte que le padding soit fait \"on batch\"\n",
        "    # Le tout devrait se passer dans la fonction collate_examples\n",
        "    train_loader = DataLoader(vectorized_train, batch_size=128, shuffle=True, collate_fn=collate_examples)\n",
        "    test_loader = DataLoader(vectorized_test, batch_size=128, collate_fn=collate_examples)\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"2 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "    # Train WordClassifierHandlingPadding\n",
        "    network = WordClassifierHandlingPadding(char_to_idx, 10, 10, len(class_to_idx))\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"3 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM/2Ggd9ceA/gjWRuK1+KF/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}