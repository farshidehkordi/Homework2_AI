{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farshidehkordi/Homework2_AI/blob/main/TP2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djuzPz1AXKEF",
        "outputId": "bc64a6d8-3043-410c-d126-c79e2050f71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# List files in the current directory\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ni9yeOHXUPk",
        "outputId": "46ac29f5-da0b-4365-b487-d6d43f544cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./codeTP2.zip\n",
            "   creating: code/data-q2/\n",
            "  inflating: code/data-q2/test.txt   \n",
            "  inflating: code/data-q2/train.txt  \n",
            "  inflating: code/q2-RNN.py          \n",
            "  inflating: code/question1.py       \n"
          ]
        }
      ],
      "source": [
        "!unzip ./codeTP2.zip\n",
        "# extract files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0pSnbdcZztO",
        "outputId": "4dfa6674-e1f7-4190-ca7f-ad168c63384d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting poutyne\n",
            "  Downloading Poutyne-1.17.1-py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.5/213.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from poutyne) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from poutyne) (2.2.1+cu121)\n",
            "Collecting torchmetrics (from poutyne)\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->poutyne)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->poutyne)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->poutyne)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->poutyne)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->poutyne)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->poutyne)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->poutyne)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->poutyne)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->poutyne)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->poutyne)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->poutyne)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->poutyne) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->poutyne)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics->poutyne) (24.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->poutyne)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics->poutyne) (67.7.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->poutyne) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->poutyne) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, poutyne\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 poutyne-1.17.1 torchmetrics-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install poutyne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dp4oOG0ltR5"
      },
      "source": [
        "Initial architecture with the bidirectional WordClassifier class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "\n",
        "# Define embedding_size and hidden_size\n",
        "embedding_size = 10\n",
        "hidden_size = 10\n",
        "\n",
        "\n",
        "# bidirectional\n",
        "class WordClassifier(nn.Module):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes, dropout_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_size, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout_prob)  # Dropout layer\n",
        "        self.mapping_layer = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embedding(inputs)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)  # Apply dropout\n",
        "        logits = self.mapping_layer(last_output)\n",
        "        return logits\n",
        "\n",
        "class WordClassifierHandlingPadding(WordClassifier):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__(vocab, embedding_size, hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # If inputs is a tuple, assume it contains sequences and lengths\n",
        "        if isinstance(inputs, tuple):\n",
        "            sequences, lengths = inputs\n",
        "        # If inputs is not a tuple, assume it contains only sequences and compute lengths\n",
        "        else:\n",
        "            sequences = inputs\n",
        "            lengths = torch.count_nonzero(sequences, dim=1).cpu().tolist()\n",
        "\n",
        "        # Embed the sequences\n",
        "        embedded = self.embedding(sequences)  # Shape: (batch_size, seq_len, embedding_size)\n",
        "\n",
        "        # Pack the embedded sequences to handle padding\n",
        "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass the packed sequences through the RNN\n",
        "        packed_output, _ = self.rnn(packed_embedded)\n",
        "\n",
        "        # Unpack the packed output\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        # Take the last output of each sequence\n",
        "        last_output = output[:, -1, :]\n",
        "\n",
        "        # Pass the last output through the mapping layer\n",
        "        logits = self.mapping_layer(last_output)  # Shape: (batch_size, num_classes)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def vectorize_dataset(dataset, char_to_idx, class_to_idx):\n",
        "    vectorized_dataset = list()\n",
        "    for word, lang in dataset:\n",
        "        label = class_to_idx[lang]\n",
        "        vectorized_word = list()\n",
        "        for char in word:\n",
        "            # Check if the character is in the vocabulary\n",
        "            if char in char_to_idx:\n",
        "                vectorized_word.append(char_to_idx[char])\n",
        "            else:\n",
        "                # If the character is not in the vocabulary, use the unknown token index\n",
        "                vectorized_word.append(char_to_idx[\"<unk>\"])\n",
        "        vectorized_dataset.append((vectorized_word, label))\n",
        "    return vectorized_dataset\n",
        "\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    examples = list()\n",
        "    with open(filename) as fhandle:\n",
        "        for line in fhandle:\n",
        "            examples.append(line[:-1].split())\n",
        "    return examples\n",
        "\n",
        "\n",
        "def create_indexes(examples):\n",
        "    char_to_idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    class_to_idx = {}\n",
        "\n",
        "    for word, lang in examples:\n",
        "        if lang not in class_to_idx:\n",
        "            class_to_idx[lang] = len(class_to_idx)\n",
        "        for char in word:\n",
        "            if char not in char_to_idx:\n",
        "                char_to_idx[char] = len(char_to_idx)\n",
        "    return char_to_idx, class_to_idx\n",
        "\n",
        "\n",
        "def make_max_padded_dataset(dataset):\n",
        "    max_length = max([len(w) for w, l in dataset])\n",
        "    tensor_dataset = torch.zeros((len(dataset), max_length), dtype=torch.long)\n",
        "    labels = list()\n",
        "    for i, (word, label) in enumerate(dataset):\n",
        "        tensor_dataset[i, :len(word)] = torch.LongTensor(word)\n",
        "        labels.append(label)\n",
        "    return tensor_dataset, torch.LongTensor(labels)\n",
        "\n",
        "\n",
        "\n",
        "def collate_examples(samples, padding_value=0):\n",
        "    # Find the maximum sequence length within the batch\n",
        "    max_len = max(len(word) for word, _ in samples)\n",
        "\n",
        "    padded_sequences = []\n",
        "    labels = []\n",
        "\n",
        "    # Pad each sequence to have the same length as the longest sequence in the batch\n",
        "    for word, label in samples:\n",
        "        # Pad the sequence to match the maximum length\n",
        "        padded_sequence = torch.tensor(word + [padding_value] * (max_len - len(word)), dtype=torch.long)\n",
        "        padded_sequences.append(padded_sequence)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert the lists to PyTorch tensors\n",
        "    padded_sequences_tensor = torch.stack(padded_sequences)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return padded_sequences_tensor, labels_tensor\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    batch_size = 128\n",
        "    training_set = load_data(\"./code/data-q2/train.txt\")\n",
        "    test_set = load_data(\"./code/data-q2/test.txt\")\n",
        "\n",
        "    char_to_idx, class_to_idx = create_indexes(training_set)\n",
        "\n",
        "    vectorized_train = vectorize_dataset(training_set, char_to_idx, class_to_idx)\n",
        "    vectorized_test = vectorize_dataset(test_set, char_to_idx, class_to_idx)\n",
        "\n",
        "    X_train, y_train = make_max_padded_dataset(vectorized_train)\n",
        "    X_test, y_test = make_max_padded_dataset(vectorized_test)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    # Define the network\n",
        "    network = WordClassifier(char_to_idx, embedding_size, hidden_size, len(class_to_idx), dropout_prob=0.7)\n",
        "\n",
        "    # Define optimizer with weight decay\n",
        "    optimizer = SGD(network.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "    # 1: Créez un réseau simple qui prend en entré des exemples de longueur fixes (max length)\n",
        "    network = WordClassifier(char_to_idx, 10, 10, len(class_to_idx))\n",
        "    model = Model(network, 'sgd', 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"1 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoyjJvzyoAYG",
        "outputId": "ba7920e2-47b8-43b7-b3c3-e9fdc2bf14d7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 Train steps: 1696 26.64s loss: 2.297778 acc: 11.963394                                \n",
            "Epoch: 2/5 Train steps: 1696 26.92s loss: 2.292957 acc: 12.812656                                \n",
            "Epoch: 3/5 Train steps: 1696 29.52s loss: 2.291645 acc: 13.009419                                \n",
            "Epoch: 4/5 Train steps: 1696 27.20s loss: 2.291035 acc: 13.029694                                \n",
            "Epoch: 5/5 Train steps: 1696 26.10s loss: 2.290884 acc: 13.034763                                \n",
            "Test steps: 424 2.14s test_loss: 2.291269 test_acc: 13.042356                                \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:1 - Loss: 2.291268605428655\tAcc:13.042356324309239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding the collate_examples function"
      ],
      "metadata": {
        "id": "DYwsy2KXn2Vf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932c046f-3d00-4cd7-e627-3e981d5b3f96",
        "id": "Vwc_4EzonCmw"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 Train steps: 1696 20.62s loss: 2.303213 acc: 11.609496                                \n",
            "Epoch: 2/5 Train steps: 1696 20.31s loss: 2.292226 acc: 13.107109                                \n",
            "Epoch: 3/5 Train steps: 1696 20.44s loss: 2.098547 acc: 19.032588                                \n",
            "Epoch: 4/5 Train steps: 1696 21.51s loss: 2.180794 acc: 16.213389                                \n",
            "Epoch: 5/5 Train steps: 1696 20.14s loss: 1.986190 acc: 21.363796                                \n",
            "Test steps: 424 2.53s test_loss: 1.786515 test_acc: 25.050688                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:2 - Loss: 1.7865145067761359\tAcc:25.05068751077908\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.optim import SGD\n",
        "\n",
        "\n",
        "# Define embedding_size and hidden_size\n",
        "embedding_size = 10\n",
        "hidden_size = 10\n",
        "\n",
        "\n",
        "# bidirectional\n",
        "class WordClassifier(nn.Module):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes, dropout_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(vocab), embedding_size, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout_prob)  # Dropout layer\n",
        "        self.mapping_layer = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embedding(inputs)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)  # Apply dropout\n",
        "        logits = self.mapping_layer(last_output)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "class WordClassifierHandlingPadding(WordClassifier):\n",
        "    def __init__(self, vocab, embedding_size, hidden_size, num_classes):\n",
        "        super().__init__(vocab, embedding_size, hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # If inputs is a tuple, assume it contains sequences and lengths\n",
        "        if isinstance(inputs, tuple):\n",
        "            sequences, lengths = inputs\n",
        "        # If inputs is not a tuple, assume it contains only sequences and compute lengths\n",
        "        else:\n",
        "            sequences = inputs\n",
        "            lengths = torch.count_nonzero(sequences, dim=1).cpu().tolist()\n",
        "\n",
        "        # Embed the sequences\n",
        "        embedded = self.embedding(sequences)  # Shape: (batch_size, seq_len, embedding_size)\n",
        "\n",
        "        # Pack the embedded sequences to handle padding\n",
        "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass the packed sequences through the RNN\n",
        "        packed_output, _ = self.rnn(packed_embedded)\n",
        "\n",
        "        # Unpack the packed output\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        # Take the last output of each sequence\n",
        "        last_output = output[:, -1, :]\n",
        "\n",
        "        # Pass the last output through the mapping layer\n",
        "        logits = self.mapping_layer(last_output)  # Shape: (batch_size, num_classes)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def vectorize_dataset(dataset, char_to_idx, class_to_idx):\n",
        "    vectorized_dataset = list()\n",
        "    for word, lang in dataset:\n",
        "        label = class_to_idx[lang]\n",
        "        vectorized_word = list()\n",
        "        for char in word:\n",
        "            # Check if the character is in the vocabulary\n",
        "            if char in char_to_idx:\n",
        "                vectorized_word.append(char_to_idx[char])\n",
        "            else:\n",
        "                # If the character is not in the vocabulary, use the unknown token index\n",
        "                vectorized_word.append(char_to_idx[\"<unk>\"])\n",
        "        vectorized_dataset.append((vectorized_word, label))\n",
        "    return vectorized_dataset\n",
        "\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    examples = list()\n",
        "    with open(filename) as fhandle:\n",
        "        for line in fhandle:\n",
        "            examples.append(line[:-1].split())\n",
        "    return examples\n",
        "\n",
        "\n",
        "def create_indexes(examples):\n",
        "    char_to_idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    class_to_idx = {}\n",
        "\n",
        "    for word, lang in examples:\n",
        "        if lang not in class_to_idx:\n",
        "            class_to_idx[lang] = len(class_to_idx)\n",
        "        for char in word:\n",
        "            if char not in char_to_idx:\n",
        "                char_to_idx[char] = len(char_to_idx)\n",
        "    return char_to_idx, class_to_idx\n",
        "\n",
        "\n",
        "def make_max_padded_dataset(dataset):\n",
        "    max_length = max([len(w) for w, l in dataset])\n",
        "    tensor_dataset = torch.zeros((len(dataset), max_length), dtype=torch.long)\n",
        "    labels = list()\n",
        "    for i, (word, label) in enumerate(dataset):\n",
        "        tensor_dataset[i, :len(word)] = torch.LongTensor(word)\n",
        "        labels.append(label)\n",
        "    return tensor_dataset, torch.LongTensor(labels)\n",
        "\n",
        "\n",
        "\n",
        "def collate_examples(samples, padding_value=0):\n",
        "    # Find the maximum sequence length within the batch\n",
        "    max_len = max(len(word) for word, _ in samples)\n",
        "\n",
        "    padded_sequences = []\n",
        "    labels = []\n",
        "\n",
        "    # Pad each sequence to have the same length as the longest sequence in the batch\n",
        "    for word, label in samples:\n",
        "        # Pad the sequence to match the maximum length\n",
        "        padded_sequence = torch.tensor(word + [padding_value] * (max_len - len(word)), dtype=torch.long)\n",
        "        padded_sequences.append(padded_sequence)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert the lists to PyTorch tensors\n",
        "    padded_sequences_tensor = torch.stack(padded_sequences)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return padded_sequences_tensor, labels_tensor\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    batch_size = 128\n",
        "    training_set = load_data(\"./code/data-q2/train.txt\")\n",
        "    test_set = load_data(\"./code/data-q2/test.txt\")\n",
        "\n",
        "    char_to_idx, class_to_idx = create_indexes(training_set)\n",
        "\n",
        "    vectorized_train = vectorize_dataset(training_set, char_to_idx, class_to_idx)\n",
        "    vectorized_test = vectorize_dataset(test_set, char_to_idx, class_to_idx)\n",
        "\n",
        "    X_train, y_train = make_max_padded_dataset(vectorized_train)\n",
        "    X_test, y_test = make_max_padded_dataset(vectorized_test)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Define the network\n",
        "    # Define the network\n",
        "    network = WordClassifier(char_to_idx, embedding_size, hidden_size, len(class_to_idx), dropout_prob=0.7)\n",
        "\n",
        "    # Define optimizer with weight decay\n",
        "    optimizer = SGD(network.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "    # 2: Ensure padding is done \"on batch\"\n",
        "    train_loader = DataLoader(vectorized_train, batch_size=128, shuffle=True, collate_fn=collate_examples)\n",
        "    test_loader = DataLoader(vectorized_test, batch_size=128, collate_fn=collate_examples)\n",
        "    model = Model(network, optimizer, 'cross_entropy', batch_metrics=['accuracy'])\n",
        "    model.fit_generator(train_loader, epochs=5)\n",
        "    loss, acc = model.evaluate_generator(test_loader)\n",
        "    logging.info(\"2 - Loss: {}\\tAcc:{}\".format(loss, acc))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWoQqnq-gTM9"
      },
      "source": [
        "Added WordClassifierHandlingPadding function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIklQzmsiVaZ",
        "outputId": "8368afc7-9436-4c9d-c44a-efd17c7b041a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:    1/1000 Train steps: 1696 2m12.48s loss: 2.315458 acc: 13.302490                              \n",
            "Epoch:    2/1000 Train steps: 1696 4m14.75s loss: 2.322811 acc: 13.244890                              \n",
            "Epoch:    3/1000 Train steps: 1696 4m15.13s loss: 2.324293 acc: 13.230144                              \n",
            "Epoch:    4/1000 Train steps: 1696 4m14.77s loss: 2.323531 acc: 13.263322                              \n",
            "Epoch:    5/1000 Train steps: 1696 4m8.44s loss: 2.323611 acc: 13.221849                              \n",
            "Epoch:    6/1000 Train steps: 1696 4m10.60s loss: 2.320723 acc: 13.224614                              \n",
            "Epoch:    7/1000 Train steps: 1696 4m9.83s loss: 2.316950 acc: 13.317697                               \n",
            "Epoch:    8/1000 Train steps: 1696 4m9.82s loss: 2.320228 acc: 13.284519                              \n",
            "Epoch:    9/1000 Step:   91/1696   5.37% |█                   |ETA: 3m47.15s loss: 2.409532 acc: 12.500000"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from poutyne.framework import Model\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# Define hyperparameters\n",
        "embedding_size = 50\n",
        "hidden_size = 100\n",
        "learning_rate = 0.01\n",
        "weight_decay = 1e-4\n",
        "num_epochs = 20\n",
        "batch_size = 128\n",
        "gradient_clip = 1.0\n",
        "\n",
        "\n",
        "class WordClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, num_classes, dropout_prob):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.mapping_layer = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embedding(inputs)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        logits = self.mapping_layer(last_output)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class WordClassifierHandlingPadding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, num_classes, dropout_prob):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
        "        self.rnn_padding = nn.RNN(embedding_size, hidden_size, batch_first=True, bidirectional=True, num_layers=2)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.mapping_layer = nn.Linear(hidden_size * 2, num_classes)\n",
        "        self.batch_norm = nn.BatchNorm1d(embedding_size)\n",
        "\n",
        "        # Initialize the weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize embedding layer\n",
        "        xavier_uniform_(self.embedding.weight.data)\n",
        "\n",
        "        # Initialize RNN weights\n",
        "        for layer in self.rnn_padding._all_weights:\n",
        "            for param in layer:\n",
        "                if 'weight' in param:\n",
        "                    xavier_uniform_(getattr(self.rnn_padding, param))\n",
        "\n",
        "        # Initialize linear layer\n",
        "        xavier_uniform_(self.mapping_layer.weight.data)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if isinstance(inputs, tuple):\n",
        "            sequences, lengths = inputs\n",
        "        else:\n",
        "            sequences = inputs\n",
        "            lengths = torch.count_nonzero(sequences, dim=1).cpu().tolist()\n",
        "\n",
        "        embedded = self.embedding(inputs)\n",
        "        embedded = self.batch_norm(embedded.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, _ = self.rnn_padding(packed_embedded)\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        logits = self.mapping_layer(last_output)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    examples = []\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            examples.append(line.strip().split())\n",
        "    return examples\n",
        "\n",
        "\n",
        "def create_indexes(examples):\n",
        "    char_to_idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    class_to_idx = {}\n",
        "    for word, lang in examples:\n",
        "        class_to_idx.setdefault(lang, len(class_to_idx))\n",
        "        for char in word:\n",
        "            char_to_idx.setdefault(char, len(char_to_idx))\n",
        "    return char_to_idx, class_to_idx\n",
        "\n",
        "\n",
        "def vectorize_dataset(dataset, char_to_idx, class_to_idx):\n",
        "    vectorized_dataset = []\n",
        "    for word, lang in dataset:\n",
        "        label = class_to_idx.get(lang, 0)\n",
        "        vectorized_word = [char_to_idx.get(char, 1) for char in word]\n",
        "        vectorized_dataset.append((vectorized_word, label))\n",
        "    return vectorized_dataset\n",
        "\n",
        "\n",
        "def make_max_padded_dataset(dataset):\n",
        "    max_length = max(len(w) for w, _ in dataset)\n",
        "    tensor_dataset = torch.zeros((len(dataset), max_length), dtype=torch.long)\n",
        "    labels = []\n",
        "    for i, (word, label) in enumerate(dataset):\n",
        "        tensor_dataset[i, :len(word)] = torch.LongTensor(word)\n",
        "        labels.append(label)\n",
        "    return tensor_dataset, torch.LongTensor(labels)\n",
        "\n",
        "\n",
        "def main():\n",
        "    training_set = load_data(\"./code/data-q2/train.txt\")\n",
        "    test_set = load_data(\"./code/data-q2/test.txt\")\n",
        "\n",
        "    char_to_idx, class_to_idx = create_indexes(training_set)\n",
        "\n",
        "    vectorized_train = vectorize_dataset(training_set, char_to_idx, class_to_idx)\n",
        "    vectorized_test = vectorize_dataset(test_set, char_to_idx, class_to_idx)\n",
        "\n",
        "    X_train, y_train = make_max_padded_dataset(vectorized_train)\n",
        "    X_test, y_test = make_max_padded_dataset(vectorized_test)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Define the network\n",
        "    network = WordClassifierHandlingPadding(len(char_to_idx), embedding_size, hidden_size, len(class_to_idx),\n",
        "                                            dropout_prob=0.7)\n",
        "\n",
        "    # Define the optimizer and scheduler\n",
        "    optimizer = Adam(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "    # Create the Poutyne Model\n",
        "    model = Model(network, optimizer, 'cross_entropy', batch_metrics=['accuracy'])\n",
        "\n",
        "   # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.fit_generator(train_loader)\n",
        "\n",
        "        loss, acc = model.evaluate_generator(test_loader)\n",
        "        logging.info(\"Epoch: {}/{} Loss: {:.4f} Accuracy: {:.4f}\".format(epoch + 1, num_epochs, loss, acc))\n",
        "        scheduler.step(loss)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "    # Set up logging\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPnKmI7W0JZCqp69R1MbPBt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}