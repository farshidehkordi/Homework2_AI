{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farshidehkordi/Homework2_AI/blob/main/TP2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89_mOhnUO-X8",
        "outputId": "50d57dd6-7e17-47be-c757-f02df74c98c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7RrmwUaHqdn",
        "outputId": "65011170-ed46-4e0d-b187-795e0fd5f85b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'drive', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# List files in the current directory\n",
        "print(os.listdir())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEmR21qwHxwu",
        "outputId": "9c20aa38-d522-4a78-cf0b-66c7d5bfe18e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'drive', 'sample_data']\n",
            "File not found.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# List files in the current directory\n",
        "print(os.listdir())\n",
        "\n",
        "# Check if the uploaded file exists\n",
        "if 'CUB_200_2011.tar' in os.listdir():\n",
        "    print(\"File exists!\")\n",
        "else:\n",
        "    print(\"File not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WYso2pkVAsj6"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "\n",
        "# open file\n",
        "file = tarfile.open('/content/drive/MyDrive/CUB_200_2011.tar')\n",
        "\n",
        "\n",
        "\n",
        "# extract files\n",
        "file.extractall('./CUB_dataset')\n",
        "\n",
        "# close file\n",
        "file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MYGMtrdCVVqu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.utils.data as td\n",
        "import torchvision as tv\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "# Function to create a directory if it doesn't exist\n",
        "def make_dir(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        os.makedirs(file_path)\n",
        "\n",
        "# Function to separate images into training and test sets\n",
        "def separate_train_test(dataset_path, train_path, test_path):\n",
        "    class_index = 1\n",
        "    for classname in sorted(os.listdir(dataset_path)):\n",
        "        if classname.startswith('.'):\n",
        "            continue\n",
        "        make_dir(os.path.join(train_path, classname))\n",
        "        make_dir(os.path.join(test_path, classname))\n",
        "        i = 0\n",
        "        for file in sorted(os.listdir(os.path.join(dataset_path, classname))):\n",
        "            if file.startswith('.'):\n",
        "                continue\n",
        "            file_path = os.path.join(dataset_path, classname, file)\n",
        "            if i < 15:  # Number of images for the test set\n",
        "                copyfile(file_path, os.path.join(test_path, classname, file))\n",
        "            else:\n",
        "                copyfile(file_path, os.path.join(train_path, classname, file))\n",
        "            i += 1\n",
        "        class_index += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EOkBGgvhbbiT"
      },
      "outputs": [],
      "source": [
        "dataset_path = './CUB_dataset/CUB_200_2011/images'\n",
        "train_path = './CUB_dataset/train'\n",
        "test_path  = './CUB_dataset/test'\n",
        "\n",
        "# Call the function to separate the dataset into training and test sets\n",
        "separate_train_test(dataset_path, train_path, test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZzdt9FYgkQV"
      },
      "source": [
        "ResNet18 using random initialization use the values from the training dataset to normalize the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opOsVHafP9wt",
        "outputId": "0eeb1e6b-5030-477a-9d3c-ed45177e5852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean values for each channel: tensor([0.4875, 0.5013, 0.4342])\n",
            "Standard deviation values for each channel: tensor([0.1749, 0.1738, 0.1861])\n",
            "Epoch [1/10], Loss: 5.301259991038929\n",
            "Epoch [2/10], Loss: 5.129460662495006\n",
            "Epoch [3/10], Loss: 4.960758075714112\n",
            "Epoch [4/10], Loss: 4.817396018288353\n",
            "Epoch [5/10], Loss: 4.677038718136874\n",
            "Epoch [6/10], Loss: 4.517424056313255\n",
            "Epoch [7/10], Loss: 4.3715684778040105\n",
            "Epoch [8/10], Loss: 4.262436491359364\n",
            "Epoch [9/10], Loss: 4.130862787420099\n",
            "Epoch [10/10], Loss: 4.034660411314531\n",
            "Accuracy on test set: 8.30%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet18\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                # Resize images to 224x224 pixels\n",
        "    transforms.ToTensor(),                         # Convert images to PyTorch tensors\n",
        "])\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Calculate mean and standard deviation from the training dataset\n",
        "mean = torch.zeros(3)  # Initialize mean tensor\n",
        "std = torch.zeros(3)   # Initialize standard deviation tensor\n",
        "\n",
        "for image, _ in train_dataset:\n",
        "    # Add pixel values of each channel to the mean tensor\n",
        "    mean += torch.mean(image, dim=(1, 2))\n",
        "    # Add standard deviation of each channel to the std tensor\n",
        "    std += torch.std(image, dim=(1, 2))\n",
        "\n",
        "# Calculate mean and standard deviation for the entire dataset\n",
        "mean /= len(train_dataset)\n",
        "std /= len(train_dataset)\n",
        "\n",
        "print(\"Mean values for each channel:\", mean)\n",
        "print(\"Standard deviation values for each channel:\", std)\n",
        "\n",
        "# Define data transformations with normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                # Resize images to 224x224 pixels\n",
        "    transforms.ToTensor(),                         # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean, std)                # Normalize images using computed mean and std\n",
        "])\n",
        "\n",
        "# Update the datasets with the new transformations\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the ResNet18 model with random initialization\n",
        "model =resnet18(weights=None)\n",
        "\n",
        "# Adapt the fully connected layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet18 using random initialization use the values from ImageNet"
      ],
      "metadata": {
        "id": "-TTggguBj6ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet18\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize images\n",
        "])\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the ResNet18 model with random initialization\n",
        "model =resnet18(weights=None)\n",
        "\n",
        "# Adapt the fully connected layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDxyecgOu8wq",
        "outputId": "ec19f369-56e5-4f5e-b863-b506c7128d37"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 5.2730580954118205\n",
            "Epoch [2/10], Loss: 5.052634617198597\n",
            "Epoch [3/10], Loss: 4.857225154529918\n",
            "Epoch [4/10], Loss: 4.69368781003085\n",
            "Epoch [5/10], Loss: 4.560795438939874\n",
            "Epoch [6/10], Loss: 4.443257312774659\n",
            "Epoch [7/10], Loss: 4.346550821824508\n",
            "Epoch [8/10], Loss: 4.221051257740368\n",
            "Epoch [9/10], Loss: 4.122021278901534\n",
            "Epoch [10/10], Loss: 4.016414707357233\n",
            "Accuracy on test set: 9.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEnxo366gnev"
      },
      "source": [
        "ResNet18 using pretrained model on CUB_200 dataset use the values from the training dataset to normalize the data, but freezing all the convolution parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIOFuiJS234X",
        "outputId": "276b01bd-21ef-452d-ea76-4cf2f7cb8e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean values for each channel: tensor([0.4875, 0.5013, 0.4342])\n",
            "Standard deviation values for each channel: tensor([0.1749, 0.1738, 0.1861])\n",
            "Epoch [1/10], Loss: 5.124327191439542\n",
            "Epoch [2/10], Loss: 4.386361411701549\n",
            "Epoch [3/10], Loss: 3.741461362838745\n",
            "Epoch [4/10], Loss: 3.20933765151284\n",
            "Epoch [5/10], Loss: 2.791107132651589\n",
            "Epoch [6/10], Loss: 2.4660621179233897\n",
            "Epoch [7/10], Loss: 2.2118477994745427\n",
            "Epoch [8/10], Loss: 2.0008856977115976\n",
            "Epoch [9/10], Loss: 1.8327548603578048\n",
            "Epoch [10/10], Loss: 1.687067993770946\n",
            "Accuracy on test set: 60.13%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet18 , ResNet18_Weights\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                # Resize images to 224x224 pixels\n",
        "    transforms.ToTensor(),                         # Convert images to PyTorch tensors\n",
        "])\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Calculate mean and standard deviation from the training dataset\n",
        "mean = torch.zeros(3)  # Initialize mean tensor\n",
        "std = torch.zeros(3)   # Initialize standard deviation tensor\n",
        "\n",
        "for image, _ in train_dataset:\n",
        "    # Add pixel values of each channel to the mean tensor\n",
        "    mean += torch.mean(image, dim=(1, 2))\n",
        "    # Add standard deviation of each channel to the std tensor\n",
        "    std += torch.std(image, dim=(1, 2))\n",
        "\n",
        "# Calculate mean and standard deviation for the entire dataset\n",
        "mean /= len(train_dataset)\n",
        "std /= len(train_dataset)\n",
        "\n",
        "print(\"Mean values for each channel:\", mean)\n",
        "print(\"Standard deviation values for each channel:\", std)\n",
        "\n",
        "# Define data transformations with normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                # Resize images to 224x224 pixels\n",
        "    transforms.ToTensor(),                         # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean, std)                # Normalize images using computed mean and std\n",
        "])\n",
        "\n",
        "# Update the datasets with the new transformations\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the ResNet18 model with pre-trained weights\n",
        "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze conv1 and bn1 parameters\n",
        "for param in model.conv1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.bn1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Freeze convolution and batch norm parameters within each BasicBlock\n",
        "for layer in [model.layer1, model.layer2, model.layer3, model.layer4]:\n",
        "    for block in layer:\n",
        "        for param in block.conv1.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in block.bn1.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in block.conv2.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in block.bn2.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "# Replace the fully connected layer with a new one (unfrozen)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdWq61L72_K2"
      },
      "source": [
        "Resnet18 using pretraind model for Freeze all layers use the values from ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize images\n",
        "])\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Load the pre-trained ResNet model\n",
        "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze conv1 and bn1 parameters\n",
        "for param in model.conv1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.bn1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Freeze convolution and batch norm parameters within each BasicBlock\n",
        "for layer in [model.layer1, model.layer2, model.layer3, model.layer4]:\n",
        "    for block in layer:\n",
        "        for param in block.conv1.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in block.bn1.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in block.conv2.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in block.bn2.parameters():\n",
        "            param.requires_grad = False\n",
        "# Adapt the fully connected layer to match the number of classes in your dataset\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, len(train_dataset.classes))  # Change the output size of the fully connected layer\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAR7XEzUHKI6",
        "outputId": "bc3b0096-be0b-432b-da01-d3b6c5618b69"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 5.112201843261719\n",
            "Epoch [2/10], Loss: 4.36550894390453\n",
            "Epoch [3/10], Loss: 3.7233505916595457\n",
            "Epoch [4/10], Loss: 3.1957089918309993\n",
            "Epoch [5/10], Loss: 2.786312924298373\n",
            "Epoch [6/10], Loss: 2.4620915985107423\n",
            "Epoch [7/10], Loss: 2.2132579729773783\n",
            "Epoch [8/10], Loss: 2.001765824664723\n",
            "Epoch [9/10], Loss: 1.8350953769683838\n",
            "Epoch [10/10], Loss: 1.692729663848877\n",
            "Accuracy on test set: 59.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv822k79FNxx"
      },
      "source": [
        "ResNet18 using the pre-trained model use the values from the training dataset to normalize the data, but only freezing the\n",
        "parameters in \"layer1\" on CUB_200 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeZfZO2LFMDp",
        "outputId": "c3c0becf-8e57-4aa1-8b44-262cbbafc916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean values for each channel: tensor([0.4875, 0.5013, 0.4342])\n",
            "Standard deviation values for each channel: tensor([0.1749, 0.1738, 0.1861])\n",
            "Epoch [1/10], Loss: 4.814942211671309\n",
            "Epoch [2/10], Loss: 3.3240532094782047\n",
            "Epoch [3/10], Loss: 2.4021008071032437\n",
            "Epoch [4/10], Loss: 1.8132276994531804\n",
            "Epoch [5/10], Loss: 1.3920872846516696\n",
            "Epoch [6/10], Loss: 1.0875356143171138\n",
            "Epoch [7/10], Loss: 0.8621930757435885\n",
            "Epoch [8/10], Loss: 0.6700554305856878\n",
            "Epoch [9/10], Loss: 0.5277180117910558\n",
            "Epoch [10/10], Loss: 0.41452188372612\n",
            "Accuracy on test set: 70.17%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18 , ResNet18_Weights\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                # Resize images to 224x224 pixels\n",
        "    transforms.ToTensor(),                         # Convert images to PyTorch tensors\n",
        "])\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Calculate mean and standard deviation from the training dataset\n",
        "mean = torch.zeros(3)  # Initialize mean tensor\n",
        "std = torch.zeros(3)   # Initialize standard deviation tensor\n",
        "\n",
        "for image, _ in train_dataset:\n",
        "    # Add pixel values of each channel to the mean tensor\n",
        "    mean += torch.mean(image, dim=(1, 2))\n",
        "    # Add standard deviation of each channel to the std tensor\n",
        "    std += torch.std(image, dim=(1, 2))\n",
        "\n",
        "# Calculate mean and standard deviation for the entire dataset\n",
        "mean /= len(train_dataset)\n",
        "std /= len(train_dataset)\n",
        "\n",
        "print(\"Mean values for each channel:\", mean)\n",
        "print(\"Standard deviation values for each channel:\", std)\n",
        "\n",
        "# Define data transformations with normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                # Resize images to 224x224 pixels\n",
        "    transforms.ToTensor(),                         # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean, std)                # Normalize images using computed mean and std\n",
        "])\n",
        "\n",
        "# Update the datasets with the new transformations\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load the pre-trained ResNet18 model\n",
        "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze conv1 and bn1 parameters in layer1\n",
        "for name, param in model.layer1.named_parameters():\n",
        "    if 'conv1' in name or 'bn1' in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "# Adapt the fully connected layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet18 using the pre-trained model use the values from the ImageNet to normalize the data, but only freezing the\n",
        "parameters in \"layer1\" on CUB_200 dataset"
      ],
      "metadata": {
        "id": "Q-GRMEIZyS_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "434d89ad-dc70-48ed-e0d8-e69ab4fd5e64",
        "id": "d8in8Mdvyefd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 88.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 4.765643106807362\n",
            "Epoch [2/10], Loss: 3.3092130062796854\n",
            "Epoch [3/10], Loss: 2.402303540923379\n",
            "Epoch [4/10], Loss: 1.8049546397816052\n",
            "Epoch [5/10], Loss: 1.3975991606712341\n",
            "Epoch [6/10], Loss: 1.089412576718764\n",
            "Epoch [7/10], Loss: 0.8604068558866328\n",
            "Epoch [8/10], Loss: 0.6792901832407171\n",
            "Epoch [9/10], Loss: 0.5245361735604026\n",
            "Epoch [10/10], Loss: 0.4097622337666425\n",
            "Accuracy on test set: 70.53%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet18 , ResNet18_Weights\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize images\n",
        "])\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load the pre-trained ResNet18 model\n",
        "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze conv1 and bn1 parameters in layer1\n",
        "for name, param in model.layer1.named_parameters():\n",
        "    if 'conv1' in name or 'bn1' in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "# Adapt the fully connected layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11ipCJ7FY-xy"
      },
      "source": [
        "ResNet18 using the pre-trained model on CUB_200 dataset use the values from the training dataset to normalize the data, but letting all the parameters (including the convolution\n",
        "layers) be adjusted by backprop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTHy1aLHADkx",
        "outputId": "8891cfe3-4357-4311-caf5-c160747e2c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 145MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 4.799591543024237\n",
            "Epoch [2/10], Loss: 3.3336651143160734\n",
            "Epoch [3/10], Loss: 2.419003915353255\n",
            "Epoch [4/10], Loss: 1.8210269156369296\n",
            "Epoch [5/10], Loss: 1.4034021405740218\n",
            "Epoch [6/10], Loss: 1.0886978756297718\n",
            "Epoch [7/10], Loss: 0.8522761667858471\n",
            "Epoch [8/10], Loss: 0.6719024848937988\n",
            "Epoch [9/10], Loss: 0.5179193783890117\n",
            "Epoch [10/10], Loss: 0.4022182380611246\n",
            "Accuracy on test set: 69.77%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize images\n",
        "])\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load the pre-trained ResNet18 model\n",
        "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Adapt the fully connected layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet18 using the pre-trained model on CUB_200 dataset use ImageNet to normalize the data, but letting all the parameters (including the convolution\n",
        "layers) be adjusted by backprop"
      ],
      "metadata": {
        "id": "sg0w_Muvy7Zx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a620e6-3b8f-4f42-e88b-195a3162b2e6",
        "id": "neh7k6tizGbL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean values for each channel: tensor([0.4875, 0.5013, 0.4342])\n",
            "Standard deviation values for each channel: tensor([0.1749, 0.1738, 0.1861])\n",
            "Epoch [1/10], Loss: 4.851184210343795\n",
            "Epoch [2/10], Loss: 3.3866179206154565\n",
            "Epoch [3/10], Loss: 2.448948258486661\n",
            "Epoch [4/10], Loss: 1.8382717362317171\n",
            "Epoch [5/10], Loss: 1.4159797137433832\n",
            "Epoch [6/10], Loss: 1.1049679084257646\n",
            "Epoch [7/10], Loss: 0.8677346444129944\n",
            "Epoch [8/10], Loss: 0.67312242009423\n",
            "Epoch [9/10], Loss: 0.5192882733995264\n",
            "Epoch [10/10], Loss: 0.4058974406935952\n",
            "Accuracy on test set: 69.80%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                # Resize images to 224x224 pixels\n",
        "    transforms.ToTensor(),                         # Convert images to PyTorch tensors\n",
        "])\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Calculate mean and standard deviation from the training dataset\n",
        "mean = torch.zeros(3)  # Initialize mean tensor\n",
        "std = torch.zeros(3)   # Initialize standard deviation tensor\n",
        "\n",
        "for image, _ in train_dataset:\n",
        "    # Add pixel values of each channel to the mean tensor\n",
        "    mean += torch.mean(image, dim=(1, 2))\n",
        "    # Add standard deviation of each channel to the std tensor\n",
        "    std += torch.std(image, dim=(1, 2))\n",
        "\n",
        "# Calculate mean and standard deviation for the entire dataset\n",
        "mean /= len(train_dataset)\n",
        "std /= len(train_dataset)\n",
        "\n",
        "print(\"Mean values for each channel:\", mean)\n",
        "print(\"Standard deviation values for each channel:\", std)\n",
        "\n",
        "# Define data transformations with normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                # Resize images to 224x224 pixels\n",
        "    transforms.ToTensor(),                         # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean, std)                # Normalize images using computed mean and std\n",
        "])\n",
        "\n",
        "# Update the datasets with the new transformations\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load the pre-trained ResNet18 model\n",
        "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Adapt the fully connected layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOwYDjijnC0n8tnAQM9ks9x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}