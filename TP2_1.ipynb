{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farshidehkordi/Homework2_AI/blob/main/TP2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89_mOhnUO-X8",
        "outputId": "469deddc-72e0-4730-d973-83a637e20bd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7RrmwUaHqdn",
        "outputId": "6a2f2baa-30d6-414a-f169-7d73a8391c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['.config', 'drive', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# List files in the current directory\n",
        "print(os.listdir())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcYuOBFUPN5S",
        "outputId": "ca49c7fe-33d9-4e03-dba9-f3f9e8ecf434"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH67zu0uPSLl",
        "outputId": "0a483578-49c1-4d3e-d671-5d7282561e61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'~$فصل-سمینار.docx'\n",
            "'01_Ou Sont Les Hommes.mp3'\n",
            "\"02_Ceux Qui N' Ont Rien.mp3\"\n",
            "'02. Koudaki.mp3'\n",
            " 2020-08-31_8-38-46.mp4\n",
            " Chapter3.mp3\n",
            " Chapter4.mp3\n",
            "'Colab Notebooks'\n",
            "'cover (1).zip'\n",
            " cover.zip\n",
            " CUB_200_2011.tar\n",
            " farshideh_kordi_12026250_18524408.pdf\n",
            " first_writing.docx\n",
            " image.JPG\n",
            " IMG_1285.JPG\n",
            "'information of university (1).gdoc'\n",
            "'information of university.docx'\n",
            "'information of university.gdoc'\n",
            " kordi.pdf\n",
            " Lhokm0.tif\n",
            " mona.mp4\n",
            " MsForm12.docx\n",
            " MsForm12.gdoc\n",
            " MWSCAS2023_CFP_v3.gdoc\n",
            " Oxford.Word.Skills.Basic-CD-ROM_IELTSMatters.com.zip\n",
            " paper.rar\n",
            " photo_۲۰۱۷-۰۴-۲۶_۱۱-۲۱-۰۸.jpg\n",
            " photo_۲۰۱۷-۰۴-۲۶_۱۱-۲۱-۰۸.jpg.gdoc\n",
            " photo_۲۰۱۷-۰۴-۲۶_۱۱-۲۱-۱۱.jpg\n",
            " photo_۲۰۱۷-۰۴-۲۶_۱۱-۲۱-۱۱.jpg.gdoc\n",
            " Power-Analysis-Attacks-Revealing-the-Secrets-of-Smart-Cards-Advances-in-Information-Security-.pdf\n",
            " power_attack.rar\n",
            " PowerTraces.rar\n",
            " RiaziMohandesiKarimi.pdf\n",
            " Tarck06.mp3\n",
            " Track05.mp3\n",
            " Track06.mp3\n",
            " Track07.mp3\n",
            " Track08.mp3\n",
            " Track09.mp3\n",
            " Track10.mp3\n",
            " Track11.mp3\n",
            " Track12.mp3\n",
            " Track13.mp3\n",
            " Track14.mp3\n",
            " Track15.mp3\n",
            " Track16.mp3\n",
            " Track17.mp3\n",
            " Track18.mp3\n",
            " Track19.mp3\n",
            " Track20.mp3\n",
            " Track21.mp3\n",
            " Track22.mp3\n",
            " Track23.mp3\n",
            " Track24.mp3\n",
            "'Untitled document.gdoc'\n",
            " Wayne+Dyer+-+Ambition+to+Meaning+-+The+Shift.torrent\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEmR21qwHxwu",
        "outputId": "fa131c58-ea65-45da-aff2-3758812dd6c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['.config', 'drive', 'sample_data']\n",
            "File not found.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# List files in the current directory\n",
        "print(os.listdir())\n",
        "\n",
        "# Check if the uploaded file exists\n",
        "if 'CUB_200_2011.tar' in os.listdir():\n",
        "    print(\"File exists!\")\n",
        "else:\n",
        "    print(\"File not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYso2pkVAsj6"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "\n",
        "# open file\n",
        "file = tarfile.open('/content/drive/MyDrive/CUB_200_2011.tar')\n",
        "\n",
        "\n",
        "\n",
        "# extract files\n",
        "file.extractall('./CUB_dataset')\n",
        "\n",
        "# close file\n",
        "file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYGMtrdCVVqu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.utils.data as td\n",
        "import torchvision as tv\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to create a directory if it doesn't exist\n",
        "def make_dir(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        os.makedirs(file_path)\n",
        "\n",
        "# Function to separate images into training and test sets\n",
        "def separate_train_test(dataset_path, train_path, test_path):\n",
        "    class_index = 1\n",
        "    for classname in sorted(os.listdir(dataset_path)):\n",
        "        if classname.startswith('.'):\n",
        "            continue\n",
        "        make_dir(os.path.join(train_path, classname))\n",
        "        make_dir(os.path.join(test_path, classname))\n",
        "        i = 0\n",
        "        for file in sorted(os.listdir(os.path.join(dataset_path, classname))):\n",
        "            if file.startswith('.'):\n",
        "                continue\n",
        "            file_path = os.path.join(dataset_path, classname, file)\n",
        "            if i < 15:  # Number of images for the test set\n",
        "                copyfile(file_path, os.path.join(test_path, classname, file))\n",
        "            else:\n",
        "                copyfile(file_path, os.path.join(train_path, classname, file))\n",
        "            i += 1\n",
        "        class_index += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOkBGgvhbbiT"
      },
      "outputs": [],
      "source": [
        "dataset_path = './CUB_dataset/CUB_200_2011/images'\n",
        "train_path = './CUB_dataset/train'\n",
        "test_path  = './CUB_dataset/test'\n",
        "\n",
        "# Call the function to separate the dataset into training and test sets\n",
        "separate_train_test(dataset_path, train_path, test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZzdt9FYgkQV"
      },
      "source": [
        "ResNet18 using random initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opOsVHafP9wt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
        "])\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the ResNet18 model with random initialization\n",
        "model =resnet18(weights=None)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEnxo366gnev"
      },
      "source": [
        "ResNet18 using pretrained model, but freezing all the convolution parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIOFuiJS234X",
        "outputId": "d99e0b55-7bc0-4b0b-b851-34002ba3fc23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:01<00:00, 38.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 5.124582522999156\n",
            "Epoch [2/10], Loss: 4.462770146456632\n",
            "Epoch [3/10], Loss: 3.947295668341897\n",
            "Epoch [4/10], Loss: 3.5296648259596393\n",
            "Epoch [5/10], Loss: 3.192490868134932\n",
            "Epoch [6/10], Loss: 2.9160115744850854\n",
            "Epoch [7/10], Loss: 2.6903091968189585\n",
            "Epoch [8/10], Loss: 2.5028984637693927\n",
            "Epoch [9/10], Loss: 2.344278058572249\n",
            "Epoch [10/10], Loss: 2.2109826317700474\n",
            "Accuracy on test set: 50.40%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet18 , ResNet18_Weights\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
        "])\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_dataset = ImageFolder(root='./CUB_dataset/train', transform=transform)\n",
        "test_dataset = ImageFolder(root='./CUB_dataset/test', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the ResNet18 model with pre-trained weights\n",
        "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "\n",
        "# Freeze all the convolution parameters\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the fully connected layer with a new one (unfrozen)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)  # Only optimize the fully connected layer\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNKj-nTCtOTf"
      },
      "source": [
        "Resnet18 using random initialization by default for ImageNet dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Load the ImageNet dataset\n",
        "train_dataset = datasets.ImageNet(root='./data', split='train', transform=transform)\n",
        "val_dataset = datasets.ImageNet(root='./data', split='val', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the ResNet-18 model with random initialization\n",
        "model = resnet18(Weights = None)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Lists to store loss and accuracy values\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}')\n",
        "\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    print(f'Validation Loss: {epoch_val_loss}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "# Plot training loss and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot validation accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YdGEBBLmbNAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet18 using the pre-trained model, but only freezing the\n",
        "parameters in \"layer1\" on CUB_200 dataset"
      ],
      "metadata": {
        "id": "pv822k79FNxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet18 , ResNet18_Weights\n",
        "\n",
        "# Load the pre-trained ResNet18 model\n",
        "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze parameters in \"layer1\"\n",
        "for name, param in model.named_parameters():\n",
        "    if 'layer1' in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Adapt the fully connected layer\n",
        "num_ftrs = model.fc.in_features\n",
        "num_classes = len(train_dataset.classes)\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "id": "MeZfZO2LFMDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet18 using the pre-trained model, but only freezing the parameters in \"layer1\" on ImageNet dataset"
      ],
      "metadata": {
        "id": "51o5iLnDJR1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageNet\n",
        "from torchvision.models import resnet18 , ResNet18_Weights\n",
        "\n",
        "# Define data transformations (including normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize images\n",
        "])\n",
        "\n",
        "# Load the ImageNet training and validation datasets\n",
        "train_dataset = ImageNet(root='./data', split='train', transform=transform, download=True)\n",
        "val_dataset = ImageNet(root='./data', split='val', transform=transform, download=True)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load the pre-trained ResNet18 model\n",
        "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze the parameters in \"layer1\"\n",
        "for param in model.layer1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the fully connected layer to adapt to the ImageNet dataset\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on validation set: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "id": "44v4EkjfJQB0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1g3uBfiQB3J37g1ztIwNvuliIvFNnzkqj",
      "authorship_tag": "ABX9TyOr5ALwXATyF3VHdScp8uxI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}